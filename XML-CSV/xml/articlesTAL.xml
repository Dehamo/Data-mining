<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE articlesTAL SYSTEM "../grammaire/articlesTAL.dtd" [
<!ENTITY article "Article de revue">
<!ENTITY communication "Communication dans un congrès">
<!ENTITY chapitre "Chapitre d'ouvrage">
<!ENTITY prepublication "Pré-publication, Document de travail">
]>
<?xml-model href="../grammaire/articlesTAL.rng" type="application/xml" schematypens="http://relaxng.org/ns/structure/1.0"?>
<articlesTAL>
<article id="art1" titre="Sequential pattern mining for discovering gene interactions and their contextual information from biomedical texts">
<auteurs>CELLIER Peggy, CHARNOIS Thierry, PLANTEVIT Marc, et al.</auteurs>
<titre_de_la_source>Journal of Biomedical Semantics</titre_de_la_source>
<date_de_publication>2015</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>http://dx.doi.org/10.1186/s13326-015-0023-3</lien>
<issn>2041-1480</issn>
<informations_complementaires>
<type_de_publication>&article;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>200112433P, 200511875R, 200012163A, 200012161Y</numero_national_de_structure_de_recherche>
<doi>10.1186/s13326-015-0023-3</doi>
<reference_hal>hal-01192959</reference_hal>
<references_archives_oai>oai:HAL:hal-01192959v1</references_archives_oai>
</informations_complementaires>
<resume>BackgroundDiscovering gene interactions and their characterizations from biological text collections is a crucial issue in bioinformatics. Indeed, text collections are large and it is very difficult for biologists to fully take benefit from this amount of knowledge. Natural Language Processing (NLP) methods have been applied to extract background knowledge from biomedical texts. Some of existing NLP approaches are based on handcrafted rules and thus are time consuming and often devoted to a specific corpus. Machine learning based NLP methods, give good results but generate outcomes that are not really understandable by a user.ResultsWe take advantage of an hybridization of data mining and natural language processing to propose an original symbolic method to automatically produce patterns conveying gene interactions and their characterizations. Therefore, our method not only allows gene interactions but also semantics information on the extracted interactions (e.g., modalities, biological contexts, interaction types) to be detected. Only limited resource is required: the text collection that is used as a training corpus. Our approach gives results comparable to the results given by state-of-the-art methods and is even better for the gene interaction detection in AIMed.ConclusionsExperiments show how our approach enables to discover interactions and their characterizations. To the best of our knowledge, there is few methods that automatically extract the interactions and also associated semantics information. The extracted gene interactions from PubMed are available through a simple web interface at https://bingotexte.greyc.fr/ webcite. The software is available at https://bingo2.greyc.fr/?q=node/22 webcite.</resume>
<thematiques>Natural language processing, Sequential pattern mining, Data mining, Information extraction, Gene interactions, Computer Science [cs]/Databases [cs.DB], Computer Science [cs]/Artificial Intelligence [cs.AI]</thematiques>
</article>
<article id="art2" titre="Analyzing TEI encoded texts with the TXM platform">
<auteurs>LAVRENTIEV Alexei, HEIDEN Serge, DECORDE Matthieu</auteurs>
<titre_de_la_source>The Linked TEI: Text Encoding in the Web. TEI Conference and Members Meeting 2013</titre_de_la_source>
<date_de_publication>2013</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://halshs.archives-ouvertes.fr/halshs-01118120</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200311862K</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>halshs-01118120</reference_hal>
<references_archives_oai>oai:HAL:halshs-01118120v1</references_archives_oai>
</informations_complementaires>
<resume>TXM (http://sf.net/projects/txm) is an open-source software platform providing tools for qualitative and quantitative content analysis of text corpora. It implements the textometric (formerly lexicometric) methods developed in France since the 1980s, as well as generally used tools of corpus search and statistical text analysis (Heiden 2010).TXM uses a TEI extension called “XML-TXM” as its native format for storing tokenized and annotated with NLP tools corpora source texts (http://sourceforge.net/apps/ mediawiki/txm/index.php?title=XML-TXM). The capacity to import and correctly analyze TEI encoded texts was one of the features requested in the original design of the platform.However, the flexibility of the TEI framework (which is its force) and the variety of encoding practices make it virtually impossible to work out a universal strategy for building a properly structured corpus (i.e. compatible with the data model of the search and analysis engines) out of an arbitrary TEI encoded text or group of texts. It should nevertheless be possible to define a subset of TEI elements that would be correctly interpreted during the various stages of the corpus import process (for example, the TEI-lite tag set), to specify the minimum requirements to the document structure and to suggest a mechanism for customization. This work is being progressively carried out by the TXM development team, but it can hardly be successful without an input from the TEI community.The goal of this paper is to present the way TXM currently deals with importing TEI encoded corpora and to discuss the ways to improve this process by interpreting TEI elements in terms of the TXM data model.</resume>
<thematiques>digital philology, TEI, Textometry, Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art3" titre="Identifying relations between scientific objects within predicate structures">
<auteurs>ROYAUTE Jean, GODBERT Elisabeth, MALIK MahdiMohamed</auteurs>
<titre_de_la_source>RANLP 2007 -- International Conference on Recent Advances in Natural Language Processing</titre_de_la_source>
<date_de_publication>2007</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01195770</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>201220259Y</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01195770</reference_hal>
<references_archives_oai>oai:HAL:hal-01195770v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art4" titre="Voltage control on a distribution network with distributed generations. Contribution of the demand flexibility">
<auteurs>HE Yujun</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2015</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://tel.archives-ouvertes.fr/tel-01323017</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source></type_de_la_source>
<numero_national_de_structure_de_recherche>199812843Y</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>tel-01323017</reference_hal>
<references_archives_oai>oai:HAL:tel-01323017v1</references_archives_oai>
</informations_complementaires>
<resume>Growth of distributed generations (DG) in actual distribution networks will bring voltage issues that cannot be fixed by conventional voltage control means. For the sake of network safety, the size of DG and load in a distribution network is limited by the network parameters.  The research described in this thesis aims to propose a voltage control strategy on distribution networks using the flexibility of demand. The voltage control means will consist of the on load tap changer (OLTC), the regulation of DG, and flexible demand. A centralized optimization of MINLP type is proposed to coordinate these voltage control means. It shows if it is not able to remove the voltage constraint with OLTC and reactive power regulation, then it must reduce the active power of DG. In order not to reduce active power of DG, the flexible demand is considered as an active source to take part in voltage control. The demand response (DR) modulation using thermal loads is thus proposed for voltage control. For the thermal load, the cold load pick-up (CLPU) effect must be taken into account in order not to affect the voltage profile after DR action. This work allows us to consider a voltage control strategy more active in smart distribution network and improve the flexibility of network.</resume>
<thematiques>Distribution networks, Voltage control, Distributed generations, Flexible response, Flexible demand, Optimization, Réseau de distribution, Réglage de tension, Productions décentralisées, Gestion de la charge, Demande flexible, Optimisation, Engineering Sciences [physics]/Other</thematiques>
</article>
<article id="art5" titre="Constraint-driven Grammar Description">
<auteurs>CRABBÉ Benoît, DUCHIER Denys, PARMENTIER Yannick, et al.</auteurs>
<titre_de_la_source>Constraints and Language</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01059206</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Ouvrage</type_de_la_source>
<numero_national_de_structure_de_recherche>200615274F</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01059206</reference_hal>
<references_archives_oai>oai:HAL:hal-01059206v1</references_archives_oai>
</informations_complementaires>
<resume>For a large number of applications in Natural Language Processing (NLP) (e.g., Dialogue Systems, Automatic Summarisation, Machine-Translation, etc.), one needs a fine-grained description of language. By fine-grained, it is generally meant that this description should precisely define the relations between the constituents of the sentence (often referred to as deep syntactic description), and, also, when possible, contain information about the meaning of the sentence (semantic representation). For such a fine-grained description of language to be processed by a computer, it is usually encoded in a mathematical framework called Formal Grammar. Here, we will consider three grammar formalisms whose expressivity lies beyond that of CFG making them suitable for the description of the syntax of several natural languages. These formalisms are Tree-Adjoining Grammar (TAG), Lexical Functional Grammar (LFG) and Property Grammar (PG). They have been used to describe several electronic grammar for e.g. English (XTAG Research Group, 2001), Chinese (Fang and King, 2007), or French (Prost, 2008). TAG will serve as a basis for illustrating one of the main issues raised when developing real-size grammatical resources, namely redundancy. Redundancy is the fact that grammar rules often share significant common substructures. This redundancy greatly impacts grammar development and maintenance (how to ensure coherence between grammar rules?). LFG and PG will serve as support formalisms in the context of cross- framework grammar engineering.</resume>
<thematiques>Formal Grammar, Constraints, Metagrammar, Computer Science [cs]/Computation and Language [cs.CL], Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art6" titre="Extracting protein-protein interactions with language modeling">
<auteurs>EBADAT Ali-Reza</auteurs>
<titre_de_la_source>Student research workshop in conjunction with RANLP 2011</titre_de_la_source>
<date_de_publication>2011</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01057652</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200012163A</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01057652</reference_hal>
<references_archives_oai>oai:HAL:hal-01057652v1</references_archives_oai>
</informations_complementaires>
<resume>In this paper, we model the corpus-based relation extraction task, namely protein- protein interaction, as a classification problem. In that framework, we first show that standard machine learning systems exploiting representations simply based on shallow linguistic information can rival state-of-the-art systems that rely on deep linguistic analysis. We also show that it is possible to obtain even more effective systems, still using these easy and reliable pieces of information, if the specifics of the extraction task and the data are taken into account. Our original method com- bining lazy learning and language mod- elling out-performs the existing systems when evaluated on the LLL2005 protein- protein interaction extraction task data.</resume>
<thematiques>Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art7" titre="Adapting a general parser to a sublanguage">
<auteurs>AUBIN Sophie, NAZARENKO Adeline, NÉDELLEC Claire</auteurs>
<titre_de_la_source>Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2005)</titre_de_la_source>
<date_de_publication>2005</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00082542</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112433P</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00082542</reference_hal>
<references_archives_oai>oai:prodinra.inra.fr:43373, oai:HAL:hal-00082542v1</references_archives_oai>
</informations_complementaires>
<resume>In this paper, we propose a method to adapt a general parser (Link Parser) to sublanguages, focusing on the parsing of texts in biology. Our main proposal is the use of terminology (identication and analysis of terms) in order to reduce the complexity of the text to be parsed. Several other strategies are explored and finally combined among which text normalization, lexicon and morpho-guessing module extensions and grammar rules adaptation. We compare the parsing results before and after these adaptations.</resume>
<thematiques>Analyse syntaxique, adaptation, sous-language, Computer Science [cs]/Computation and Language [cs.CL], Computer Science [cs]/Information Retrieval [cs.IR]</thematiques>
</article>
<article id="art8" titre="Proceedings of the COLING Workshop on Multiword Expressions: from Theory to Applications (MWE 2010)">
<auteurs>LAPORTE Eric, NAKOV Preslav, RAMISCH Carlos, et al.</auteurs>
<titre_de_la_source>COLING</titre_de_la_source>
<date_de_publication>2010</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal-upec-upem.archives-ouvertes.fr/hal-00722851</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200711886U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00722851</reference_hal>
<references_archives_oai>oai:HAL:hal-00959249v1, oai:HAL:hal-00722851v1</references_archives_oai>
</informations_complementaires>
<resume>Multiword Expressions (MWEs) are a ubiquitous component of natural languages. The automated processing of MWEs is desirable for any natural language application that involves some degree of semantic interpretation, e.g., Machine Translation, Information Extraction, and Question Answering.In spite of the recent advances in the field, there is a wide range of open problems that prevent MWE treatment techniques from full integration in current NLP systems.We thus asked for original research related but not limited to the following topics: MWE resources, hybrid approaches, domain adaptation, and multilingualism.We received 18 submissions, and we were only able to accept eight full papers for oral presentation: an acceptance rate of 44%. We further accepted four papers as posters. The regular papers were distributed in three sessions: Lexical Representation, Identification and Extraction, and Applications.The workshop also featured two invited talks, by Kyo Kageura and by Aravind K. Joshi, and a panel discussion.</resume>
<thematiques>multiword expression, Computer Science [cs]/Document and Text Processing, Computer Science [cs]/Computation and Language [cs.CL], Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art9" titre="Strategies to select examples for Active Learning with Conditional Random Fields">
<auteurs>CLAVEAU Vincent, KIJAK Ewa</auteurs>
<titre_de_la_source>Conférence TALN 2015</titre_de_la_source>
<date_de_publication>2015</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01206847</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200012163A</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01206847</reference_hal>
<references_archives_oai>oai:HAL:hal-01206847v1</references_archives_oai>
</informations_complementaires>
<resume>Nowadays, many NLP problems are modelized as supervised machine learning tasks. Consequently, the cost of the expertise needed to annotate the examples is a widespread issue. Active learning offers a framework to that issue, allowing to control the annotation cost while maximizing the classifier performance, but it relies on the key step of choosing which example will be proposed to the expert.In this paper, we examine and propose such selection strategies in the specific case of Conditional Random Fields (CRF) which are largely used in NLP. On the one hand, we propose a simple method to correct a bias of certain state-of-the-art selection techniques. On the other hand, we detail an original approach to select the examples, based on the respect of proportions in the datasets. These contributions are validated over a large range of experiments implying several tasks and datasets, including named entity recognition, chunking, phonetization, word sens disambiguation.</resume>
<thematiques>conditional random fields, active learning, semi-supervised learning, statistical test of proportion, CRF, champs aléatoires conditionnels, apprentissage actif, apprentissage semi-supervisé, test statis- tique de proportion, Computer Science [cs]/Computation and Language [cs.CL], Computer Science [cs]/Machine Learning [cs.LG], Computer Science [cs]/Artificial Intelligence [cs.AI]</thematiques>
</article>
<article id="art10" titre="Online Graph Matching">
<auteurs>GUILLAUME Bruno</auteurs>
<titre_de_la_source>Traitement Automatique des Langues Naturelles (TALN)</titre_de_la_source>
<date_de_publication>2015</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.inria.fr/hal-01188682</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>198912571S</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01188682</reference_hal>
<references_archives_oai>oai:HAL:hal-01188682v1</references_archives_oai>
</informations_complementaires>
<resume>We present an online tool for graph pattern matching in syntactically annotated corpora.</resume>
<thematiques>Dependency Syntax, Corpus, Graph matching, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art11" titre="Magnaporthe oryzae effectors MoHEG13 and MoHEG16 interfere with host infection and MoHEG13 counteracts cell death caused by Magnaporthe-NLPs in tobacco">
<auteurs>MOGGA Valérie, DELVENTHAL Rhoda, WEIDENBACH Denise, et al.</auteurs>
<titre_de_la_source>Plant Cell Reports</titre_de_la_source>
<date_de_publication>2016</date_de_publication>
<numero></numero>
<pagination>1169-1185</pagination>
<lien>http://prodinra.inra.fr/record/355730</lien>
<issn>0721-7714</issn>
<informations_complementaires>
<type_de_publication>&article;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>194517892S</numero_national_de_structure_de_recherche>
<doi>10.1007/s00299-016-1943-9</doi>
<reference_hal></reference_hal>
<references_archives_oai>oai:prodinra.inra.fr:355730</references_archives_oai>
</informations_complementaires>
<resume>Adapted pathogens are able to modulate cell responses of their hosts most likely due to the activity of secreted effector molecules thereby enabling colonisation by ostensible nonhost pathogens. It is postulated that host and nonhost pathogens of a given plant species differ in their repertoire of secreted effector molecules that are able to suppress plant resistance. We pursued the strategy of identifying novel effectors of Magnaporthe oryzae, the causal agent of blast disease, by comparing the infection process of closely related host vs. nonhost Magnaporthe species on barley (Hordeum vulgare L.). When both types of pathogen simultaneously attacked the same cell, the nonhost isolate became a successful pathogen possibly due to potent effectors secreted by the host isolate. Microarray studies led to a set of M. oryzae Hypothetical Effector Genes (MoHEGs) which were classified as Early- and LateMoHEGs according to the maximal transcript abundance during colonization of barley. Interestingly, orthologs of these MoHEGs from a nonhost pathogen were similarly regulated when investigated in a host situation, suggesting evolutionary conserved functions. Knockout mutants of MoHEG16 from the group of EarlyMoHEGs were less virulent on barley and microscopic studies revealed an attenuated transition from epidermal to mesophyll colonization. MoHEG13, a LateMoHEG, was shown to antagonize cell death induced by M. oryzae Necrosis-and ethylene-inducing-protein-1 (Nep1)-like proteins in Nicotiana benthamiana. MoHEG13 has a virulence function as a knockout mutant showed attenuated disease progression when inoculated on barley.</resume>
<thematiques>magnaporthe oryzae, barley, effector proteins, necrosis, magnaporthe oryzae, tabac, maladie des plantes</thematiques>
</article>
<article id="art12" titre="Un concordancier multi-niveaux et multimédia pour des corpus oraux">
<auteurs>BARRECA Giulia, CHRISTODOULIDES George</auteurs>
<titre_de_la_source>21e Conférence sur le Traitement automatique des Langues Naturelles (TALN 2014)</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://halshs.archives-ouvertes.fr/halshs-01078133</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112501N</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>halshs-01078133</reference_hal>
<references_archives_oai>oai:HAL:halshs-01078133v1</references_archives_oai>
</informations_complementaires>
<resume>Concordances have always played an important role in the analysis of language corpora, for studies inhumanities, literature, linguistics, translation and language teaching. However, very few of the available systemssupport multi-level queries against a richly-annotated, sound-aligned spoken corpus. The rapid growth in thedevelopment of spoken corpora, particularly for French, increases the need for scalable, high-performance solutions.We present the preliminary results of our project to develop a multi-level multimedia concordancer for spoken languagecorpora. We test our prototype on the PFC corpus of spoken French (1.5 million tokens, transcriptions aligned to theutterance level). Our tool allows researchers to query the corpus and produce concordances correlating severalannotation levels (part-of-speech tags, lemmas, annotation of phonological phenomena such as the liaison and schwa,etc.) while allowing for multi-modal access to the associated sound recordings and other data.</resume>
<thematiques>French language teaching, Corpus linguistics, Multi-level annotation, Concordance tool, Linguistique de corpus, Didactique du FLE, Annotation multi-niveaux, Concordancier, Humanities and Social Sciences</thematiques>
</article>
<article id="art13" titre="Proceedings of the Grammar Engineering Across Frameworks (GEAF) 2015 Workshop">
<auteurs>BENDER Emily, LEVIN Lori, MÜLLER Stefan, et al.</auteurs>
<titre_de_la_source>The 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</titre_de_la_source>
<date_de_publication>2015</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01181344</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200615274F</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01181344</reference_hal>
<references_archives_oai>oai:HAL:hal-01181344v1</references_archives_oai>
</informations_complementaires>
<resume>Grammar Engineering Across Frameworks (GEAF) 2015 took place on 30 July 2015 in Beijing.  This workshop  builds  on  several  previous  workshop  on  the  same  topic,  namely  GEAF  2007  at  the  LSA Linguistic Institute at Stanford, GEAF 2008 at COLING in Manchester, GEAF 2009 at ACL/IJCNLP in Singapore and HMGE 2013 at ESSLLI 2013 in Düsseldorf.Grammar engineering, the practice of developing linguistically motivated grammars in software, is an active  area  of  research  in  computational  linguistics  and  comprises  contemporary  works  across  many different theoretical frameworks.  The fruits of grammar engineering, namely linguistically motivated grammars which in many cases provide rich, detailed semantic representations, support the development of natural language technologies,  including both natural language understanding and generation,  that derive much more information from the linguistic signal than is otherwise possible.  The goal of this workshop is to bring together researchers working in grammar engineering and to advance the state of the art in this field. In addition to the nine papers included in these proceedings, the workshop featured a panel discussion on how grammar engineering can continue to be relevant in computational linguistics. The panel addressed questions such as, What are the strengths of grammars that cannot be ignored? What success stories do we have? What should be done differently? And what can we learn from other approaches?</resume>
<thematiques>formal grammar, grammar engineering, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art14" titre="Sequential Patterns of POS Labels Help to Characterize Language Acquisition">
<auteurs>TELLIER Isabelle, MAKHLOUF Zineb, DUPONT Yoann</auteurs>
<titre_de_la_source>DMNLP (ECML/PKDD Workshop)</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01140542</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200012753S</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01140542</reference_hal>
<references_archives_oai>oai:HAL:hal-01140542v1</references_archives_oai>
</informations_complementaires>
<resume>In this paper, we try to characterize various steps of the syntax acquisition of their native language by children with emerging sequential patterns of Part Of Speech (POS) labels. To achieve this goal, we first build a set of corpora from the French part of the CHILDES database. Then, we study the linguistic utterances of the children of various ages with tools coming from Natural Language Processing (morpho-syntactic labels obtained by supervised machine learning) and sequential Data Mining (emerging patterns among the sequences of morpho-syntactc labels). This work thus illustrates the interest of combining both approaches. We show that the distinct ages can be characterized by variations of proportions of morpho-syntactic labels, which are also clearly visible inside the emerging patterns.</resume>
<thematiques>POS labeling, CRF, Sequential Data Mining, emerging patterns, language acquisition, Computer Science [cs]/Machine Learning [cs.LG], Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art15" titre="BocopHJB 1.0.1 – User Guide">
<auteurs>BONNANS Frédéric, GIORGI Daphné, HEYMANN Benjamin, et al.</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2015</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.inria.fr/hal-01192610</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source></type_de_la_source>
<numero_national_de_structure_de_recherche>199712645M, 199719340P</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01192610</reference_hal>
<references_archives_oai>oai:HAL:hal-01192610v1</references_archives_oai>
</informations_complementaires>
<resume>The original Bocop package implements a local optimization method. The optimalcontrol problem is approximated by a finite dimensional optimization problem (NLP) using a timediscretization (the direct transcription approach). The NLP problem is solved by the well knownsoftware Ipopt, using sparse exact derivatives computed by Adol-C.The second package BocopHJB implements a global optimization method. Similarly to the DynamicProgramming approach, the optimal control problem is solved in two steps. First we solvethe Hamilton-Jacobi-Bellman equation satisfied by the value fonction of the problem. Then wesimulate the optimal trajectory from any chosen initial condition. The computational effort isessentially taken by the first step, whose result, the value fonction, can be stored for subsequenttrajectory simulations.</resume>
<thematiques>HJB, optimization, optimal control, stochastic control, dynamic programming, spaceship, optimal switching, Computer Science [cs]</thematiques>
</article>
<article id="art16" titre="Faire du TAL sur des données personnelles : un oxymore ?">
<auteurs>DEMAZANCOURT Hugues, COUILLAULT Alain, ADDA Gilles, et al.</auteurs>
<titre_de_la_source>TALN 2015 - 22ème Conférence sur le Traitement Automatique des Langues Naturelles / Atelier ETERNAL°</titre_de_la_source>
<date_de_publication>2015</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01271995</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>199613735B, 197217542U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01271995</reference_hal>
<references_archives_oai>oai:HAL:hal-01271995v1, oai:HAL:hal-01171519v1</references_archives_oai>
</informations_complementaires>
<resume>We present the work in progress for the version 2 of the Big Data Charter and present some aspects of the use of personal data for an industrial system embedding NLP technology.</resume>
<thematiques>Ethics, NLP, Big Data, Privacy, Computer Science [cs]/Computation and Language [cs.CL], Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art17" titre="Modeling a multi-queue network node with a fuzzy predictor">
<auteurs>JAMHOUR Edgard, DEOLIVEIRAPENNANETO ManoelCamillo, NABHEN Ricardo, et al.</auteurs>
<titre_de_la_source>Fuzzy Sets and Systems</titre_de_la_source>
<date_de_publication>2009</date_de_publication>
<numero>13</numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01168789</lien>
<issn>0165-0114</issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>199712651U</numero_national_de_structure_de_recherche>
<doi>10.1016/j.fss.2008.12.004</doi>
<reference_hal>hal-01168789</reference_hal>
<references_archives_oai>oai:HAL:hal-01168789v1</references_archives_oai>
</informations_complementaires>
<resume>Capacity planning of IP-based networks is a difficult task. Ideally, in order to estimate the maximum amount of traffic that can be carried by the network, without violating QoS requirements such as end-to-end delay and packet loss, it is necessary to determine the queue length distribution of the network nodes under different traffic conditions. When per-flow guarantees are required (e.g., VoIP traffic), it is also necessary to determine the impact of the queue behavior on the performance of individual flows. Analytical models for queue length distribution are available only for relatively simple traffic patterns. This paper proposes a generic method for building a fuzzy predictor for modeling the behavior of a DiffServ node with multiple queues. The method combines nonlinear programming (NLP) and simulation to build a fuzzy predictor capable of determining the performance of a DiffServ node subjected to both per-flow and aggregated performance guarantees. This approach does not require deriving an analytical model, and can be applied to any type of traffic. In this paper, we employ the fuzzy approach to model the behavior of a multi-queue node where (aggregated ON-OFF) VoIP traffic and (self-similar) data traffic compete for the network resources.</resume>
<thematiques>Computer Science [cs]</thematiques>
</article>
<article id="art18" titre="LGPLLR : an open source license for NLP (Natural Language Processing)">
<auteurs>PAUMIER Sébastien</auteurs>
<titre_de_la_source>Rencontres mondiales du logiciel libre</titre_de_la_source>
<date_de_publication>2009</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01168649</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200212717U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01168649</reference_hal>
<references_archives_oai>oai:HAL:hal-01168649v1</references_archives_oai>
</informations_complementaires>
<resume>We present the aims and principles of the LGPLLR open-source license for language resources</resume>
<thematiques>License, Language Resource, Open Source, Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art19" titre="Structuration automatique de flux télévisuels">
<auteurs>GUINAUDEAU Camille</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2011</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://tel.archives-ouvertes.fr/tel-00646522</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source></type_de_la_source>
<numero_national_de_structure_de_recherche>200012163A</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>tel-00646522</reference_hal>
<references_archives_oai>oai:HAL:tel-00646522v1</references_archives_oai>
</informations_complementaires>
<resume>The increasing quantity of video material available requires the implementation of automatic structuring techniques that can facilitate access to the information contained in documents, while being generic enough to be able to structure different kinds of videos. For this, we develop two kinds of thematic structuring of TV shows, linear or hierarchical, based on the automatic transcripts of the speech pronounced in the programs. These transcripts, independent of the type of documents considered, are used thanks to natural language processing (NLP) methods. The two structuring techniques, as well as the topic segmentation phase on which they rely, has led to several original contributions. First, the topic segmentation technique employed, originally developed for text, is adapted to the peculiarities of professional videos transcripts - transcription errors, limited number of repetition. The lexical cohesion criterion on which the segmentation step is based is, indeed, sensitive to these characteristics, which severely penalizes the algorithm performances. This adaptation is implemented, on the one hand by taking into account, during the lexical cohesion computation, linguistic knowledge and automatic speech recognition and signal information (semantic relations, prosody, confidence measures), and on the other hand on language model interpolation techniques. From this topic segmentation step, we propose a method for linear thematic structuring that is able to connect segments addressing similar topic. The method, based on a technique from the information retrieval domain, is adapted to the audiovisual data through prosodic cues, that help to promote prominent words in the speech, and semantic relations. Finally, we propose an exploratory work that studies different ways to adapt a linear topic segmentation algorithm to a hierarchical topic segmentation task. For this, the linear topic segmentation algorithm is modified - adjustement of the lexical cohesion computation, use of lexical chains - to reflect the distribution of the vocabulary in the document to be segmented. Experiments conducted on three corpora composed of broadcast news and reports on current affairs, manually and automatically transcribed, show that the proposed adjustments lead to improved performance of the structuring methods developed.</resume>
<thematiques>multimédia, traitement automatique des langues, reconnaissance automatique de la parole, structuration de flux télévisuels, segmentation thématique, mesures de confiance, relations sémantiques, prosodie, segmentation thématique hiérarchique, Computer Science [cs]/Multimedia [cs.MM], Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art20" titre="QAKiS: an Open Domain QA System based on Relational Patterns">
<auteurs>CABRIO Elena, COJAN Julien, MAGNINI Bernardo, et al.</auteurs>
<titre_de_la_source>International Semantic Web Conference, ISWC 2012</titre_de_la_source>
<date_de_publication>2012</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.inria.fr/hal-01171115</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>201220432L</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01171115</reference_hal>
<references_archives_oai>oai:HAL:hal-01171115v1</references_archives_oai>
</informations_complementaires>
<resume>We present QAKiS, a system for open domain Question Answering over linked data. It addresses the problem of question interpretation as a relation-based match, where fragments of the question are matched to binary relations of the triple store, using relational textual patterns automatically collected. For the demo, the relational patterns are automatically extracted from Wikipedia, while DBpedia is the RDF data set to be queried using a natural language interface.</resume>
<thematiques>NLP, Question Answering System, linked data, Dbpedia, Wikipedia, semantic web, natural language processing, Computer Science [cs]/Artificial Intelligence [cs.AI], Computer Science [cs]/Web, Computer Science [cs]/Social and Information Networks [cs.SI]</thematiques>
</article>
<article id="art21" titre="Counter-Argumentation and Discourse: A Case Study">
<auteurs>AFANTENOS Stergos, ASHER Nicholas</auteurs>
<titre_de_la_source>Frontiers and Connections between Argumentation Theory and Natural Language Processing (ArgNLP 2014)</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01343000</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>199511949P</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01343000</reference_hal>
<references_archives_oai>oai:HAL:hal-01343000v1</references_archives_oai>
</informations_complementaires>
<resume>Despite the central role that argumentation plays in human communication, the computational linguistics community has paid relatively little attention in proposing a methodology for automatically identifying arguments and their relations in texts. Argumentation is intimately related with discourse structure, since an argument often spans more than one phrase, forming thus an entity with its own coherent internal structure. Moreover, arguments are linked between them either with a support, an attack or a rebuttal relation. Those argumentation relations are often realized via a discourse relation. Unfortunately, most of the discourse representation theories use trees in order to represent discourse, a format which is incapable of representing phenomena such as long distance attachments and crossed dependencies which are crucial for argumentation. A notable exception is Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides, 2003). In this paper we show how SDRT can help identify arguments and their relations. We use counter-argumentation as our case study following Apotheloz (1989)and Amgoud and Prade (2012) showing how the identification of the discourse structure can greatly benefit the identification of the argumentation structure.</resume>
<thematiques>Argumentation, Computer Science [cs]/Artificial Intelligence [cs.AI], Computer Science [cs]/Machine Learning [cs.LG], Computer Science [cs]/Logic in Computer Science [cs.LO], Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art22" titre="Automatic Acquisition of Semantic Relationships from Morphological Relatedness">
<auteurs>BERNHARD Delphine</auteurs>
<titre_de_la_source>Proceedings of the 5th International Conference on NLP, FinTAL 2006</titre_de_la_source>
<date_de_publication>2006</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00800365</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>199511969L</numero_national_de_structure_de_recherche>
<doi>10.1007/11816508_14</doi>
<reference_hal>hal-00800365</reference_hal>
<references_archives_oai>oai:HAL:hal-00800365v1</references_archives_oai>
</informations_complementaires>
<resume>Semantic relationships like specialisation can be acquired either by word-external methods relying on the context or word-internal methods based on lexical structure. Word segments are thus a relevant cue for the automatic acquisition of semantic relationships. We have developed an unsupervised method for morphological segmentation devised for this objective. Semantic relationships are deduced from specific morphological structures based on the segments discovered. Evaluation of the validity of the semantic relationships inferred is performed against WordNet and the NCI Thesaurus.</resume>
<thematiques>Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art23" titre="Recognition and TEI annotation of Arabic Events Using Transducers">
<auteurs>BENMESMIA Fatma, FRIBURGER Nathalie, HADDAR Kais, et al.</auteurs>
<titre_de_la_source>17th International Conference on Intelligent Text Processing and Computational Linguistics</titre_de_la_source>
<date_de_publication>2016</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01291336</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>201220254T</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01291336</reference_hal>
<references_archives_oai>oai:HAL:hal-01291336v1</references_archives_oai>
</informations_complementaires>
<resume>The recognition of Arabic Named Entity (ANE) is an important task allowing the identification and classification of relevant entities to predefined categories in the textual resources. In fact, the ANE having the category Event becomes a new challenge in NLP applications. Therefore, their appearance is clearly related to the evolution of the Web. Hence, it generates regularly new events’ articles appearing in the free resources such as Wikipedia. Nevertheless, their recognition and annotation require a powerful formalism and standard in order to have structured output. In this paper, we propose a method to recognize and to annotate ANE event. The proposed method is based on finite state trans-ducers using the TEI recommendation. These transducers are regrouped in a cas-cade generated by CaSsys tool available under Unitex linguistic platform. Our corpora are extracted from Arabic Wikipedia through the Kiwix tool. The ob-tained results are satisfactory through the calculated measures.</resume>
<thematiques>Transducer cascade, TEI annotation, Arabic Named Entity event, Arabic Wikipedia corpus, Unitex linguistic platform, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art24" titre="Annotation - the ISO (and NLP) perspective">
<auteurs>ROMARY Laurent</auteurs>
<titre_de_la_source>ECHO IT Days</titre_de_la_source>
<date_de_publication>2003</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.inria.fr/inria-00099546</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>198912571S</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>inria-00099546</reference_hal>
<references_archives_oai>oai:HAL:inria-00099546v1</references_archives_oai>
</informations_complementaires>
<resume>The talk is about annotation issues, based on the experience in the area of NLP and as chairman of ISO/TC 37/SC 4. Three questions are raised that are currently in the focus of the discussion: (1) How can we share resources? (2) How can we share tools? (3) How can we assure meaning consistency between annotations?</resume>
<thematiques>normalisation, standardization, xml, data category registry, iso 16642, répertoire de catégories de données, iso 11179, Computer Science [cs]/Other [cs.OH]</thematiques>
</article>
<article id="art25" titre="Fault Detection and Localization with Neural Principal Component Analysis">
<auteurs>OUNI Khaled, NABLI Lotfi, SIMEU-ABAZI Zineb</auteurs>
<titre_de_la_source>International Conference on Communications, Computing and Control Applications</titre_de_la_source>
<date_de_publication>2011</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00684238</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200919221H</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00684238</reference_hal>
<references_archives_oai>oai:HAL:hal-00684238v1</references_archives_oai>
</informations_complementaires>
<resume>This paper presents a detection and diagnosis fault based on Neural Non Linear Principal Component Analysis (NNLPCA) and a Partial Least Square (PLS). This method is applied on a manufactured system, and the NNLPCA approach is used to estimate the non linear component. This NNLPCA model helps to estimate the prediction error and to define data classes with and without faults. The classes associated to data with faults are isolated by applying a PLS-2. Detecting faults is realized by SPE (square prediction error) statistics method, while locating them is realized by calculating contributions.</resume>
<thematiques>NIPALS algorithm., PLS-2, NIPALS algorithm, Fault diagnosis, Neural Principal Component Analysis, Partial Least Square, Engineering Sciences [physics]/Automatic</thematiques>
</article>
<article id="art26" titre="Integration and publication of heterogeneous text-mined relationships on the Semantic Web">
<auteurs>COULET Adrien, GARTEN Yael, DUMONTIER Michel, et al.</auteurs>
<titre_de_la_source>Journal of Biomedical Semantics</titre_de_la_source>
<date_de_publication>2011</date_de_publication>
<numero>S2</numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00585215</lien>
<issn>2041-1480</issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>198912571S</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00585215</reference_hal>
<references_archives_oai>oai:HAL:hal-00585215v1</references_archives_oai>
</informations_complementaires>
<resume>Background - Advances in Natural Language Processing (NLP) techniques enable the extraction of fine-grained relationships mentioned in biomedical text. The variability and the complexity of natural language in expressing similar relationships causes the extracted relationships to be highly heterogeneous, which makes the construction of knowledge bases difficult and poses a challenge in using these for data mining or question answering. Results - We report on the semi-automatic construction of the PHARE relationship ontology (the PHArmacogenomic RElationships Ontology) consisting of 200 curated relations from over 40,000 heterogeneous relationships extracted via text-mining. These heterogeneous relations are then mapped to the PHARE ontology using synonyms, entity descriptions and hierarchies of entities and roles. Once mapped, relationships can be normalized and compared using the structure of the ontology to identify relationships that have similar semantics but different syntax. We compare and contrast the manual procedure with a fully automated approach using WordNet to quantify the degree of integration enabled by iterative curation and refinement of the PHARE ontology. The result of such integration is a repository of normalized biomedical relationships, named PHARE-KB, which can be queried using Semantic Web technologies such as SPARQL and can be visualized in the form of a biological network. Conclusions - The PHARE ontology serves as a common semantic framework to integrate more than 40,000 relationships pertinent to pharmacogenomics. The PHARE ontology forms the foundation of a knowledge base named PHARE-KB. Once populated with relationships, PHARE-KB (i) can be visualized in the form of a biological network to guide human tasks such as database curation and (ii) can be queried programmatically to guide bioinformatics applications such as the prediction of molecular interactions. PHARE is available at http://purl.bioontology.org/ontology/PHARE.</resume>
<thematiques>text mining, ontology, relationships extraction, relationships normalization, pharmacogenomics, gene-disease networks, Computer Science [cs]/Bioinformatics [q-bio.QM], Life Sciences [q-bio]/Quantitative Methods [q-bio.QM], Computer Science [cs]/Document and Text Processing, Computer Science [cs]/Artificial Intelligence [cs.AI]</thematiques>
</article>
<article id="art27" titre="Senterritoire pour la détection d’opinions liées à l’aménagement d’un territoire">
<auteurs>KERGOSIEN Eric, MAUREL Pierre, ROCHE Mathieu, et al.</auteurs>
<titre_de_la_source>Revue Internationale de Géomatique</titre_de_la_source>
<date_de_publication>2015</date_de_publication>
<numero>1</numero>
<pagination>nov-34</pagination>
<lien>http://dx.doi.org/10.3166/rig.25.11-34</lien>
<issn>1260-5875</issn>
<informations_complementaires>
<type_de_publication>&article;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>200718239Z, 199111950H</numero_national_de_structure_de_recherche>
<doi>10.3166/rig.25.11-34</doi>
<reference_hal>lirmm-01109090</reference_hal>
<references_archives_oai>oai:HAL:lirmm-01109090v1</references_archives_oai>
</informations_complementaires>
<resume>De nombreux travaux ont été réalisés en extraction d’informations et plus particulièrement en fouille de données d’opinions dans des contextes spécifiques tels que les critiques de films, les évaluations de produits commerciaux, les discours électoraux... Dans le cadre du projet SENTERRITOIRE, nous nous posons la question de l’adéquation de ces méthodes pour des documents associés à l’aménagement des territoires. Ces documents renferment différents types d’informations se rapportant à des acteurs, des opinions, des informations géographiques, et tout autre aspect lié plus généralement à la notion de territoire. Cependant, il est extrêmement difficile d’identifier puis de mettre en relation les opinions et ces informations. Dans cet article, nous décrivons la méthode semi-automatique qui combine une chaîne de traitement automatique du langage naturel et des techniques de fouilles de textes pour détecter les opinions relatives aux informations géospatiales formant un territoire, et le démonstrateur associé.</resume>
<thematiques>Information géographique, Aménagement du territoire, Visualisation d’informations, TALN, Fouille d’opinions, Computer Science [cs]/Document and Text Processing, Computer Science [cs]/Information Retrieval [cs.IR], Computer Science [cs]/Web</thematiques>
</article>
<article id="art28" titre="Specification of linguistic annotations according to corpora : from newspaper to spoken corpora">
<auteurs>ESHKOL-TARAVELLA Iris</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2015</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/tel-01250650</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source></type_de_la_source>
<numero_national_de_structure_de_recherche>201220244G</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>tel-01250650</reference_hal>
<references_archives_oai>oai:HAL:tel-01250650v1</references_archives_oai>
</informations_complementaires>
<resume>Confronté à Internet, le Traitement Automatique des Langues (TAL) a dû relever le défi que posait l’analyse de textes dialogiques écrits (blog, forum, chat, réseaux sociaux etc.) et oraux. Les recherches présentées ont, dans un premier temps, porté sur le développement de systèmes à même de repérer et d’analyser l’information à partir d’une annotation des ressources. L’approche retenue privilégie l’intégration d’indices inhérents à la nature de corpus « hors normes » afin d’améliorer les techniques de traitement automatique. La chaîne d’opérations comprend quatre étapes :(i) L’observation et l’analyse manuelle des données afin de recenser les variations dans les occurrences et d’évaluer l’ampleur des phénomènes à annoter, leur classification et l’identification de leurs marqueurs formels.(ii) La modélisation de l’information à partir d’une typologie sous la forme d’un jeu d’étiquettes ajusté à la nature du corpus.(iii) La définition de la technologie congrue (généralement, l’arbitrage entre le développement d’un nouvel outil  et l’adaptation d’un outil existant).(iv) L’implémentation du schéma d’annotation défini afin de procéder à une analyse quantitative et qualitative des résultats.L’annotation effectuée concerne les domaines de la syntaxe (étiquetage morpho-syntaxique et chunking), sémantique et/ou pragmatique (entités nommées, indices d’identification de la personne, reformulations  etc.). L’application concerne aussi bien des entretiens transcrits que des titres de cartes géographiques, des recettes d’omelette que des articles du Monde. Les méthodes utilisées varient en fonction du corpus et de la tâche traitée. L’annotation syntaxique et le repérage des segments reformulés sont fondés sur la technique d’apprentissage automatique avec les CRFs ,  le repérage des entités nommées et des indices d’identification de la personne dans les transcriptions de l’oral utilise les méthodes symboliques ,  la détection automatique des tours de parole contenant la reformulation emploie les méthodes heuristiques.  Le travail sur le français parlé et son annotation a conduit à la modélisation des caractéristiques propres à l’oral : disfluences, marqueurs discursifs, présentateurs, segmentation, commentaires personnels etc. Un autre phénomène caractéristique de l’oral, la reformulation, a fait l’objet d’une étude particulière. Le travail sur l’annotation du corpus oral, du corpus Web ou du corpus médiatique a permis de reconsidérer la notion de subjectivité qui constitue l’une des difficultés récurrentes du traitement automatique. L’étude de la subjectivité et son expression dans le discours a été poursuivie dans plusieurs des recherches menées : la subjectivité à partir des informations personnelles livrées par le locuteur, la subjectivité dans la perception et l’appropriation des lieux, la subjectivité dans les recettes de cuisine et enfin la subjectivité exprimée à travers les noms généraux.</resume>
<thematiques>Reformulation, Names entities, Anonymisation, General nouns, Toponyms, Subjective places, Subjectivity, Recipes, NLP, Oral corpus, Chunking, Annotation Morpho-syntaxique, Annotation, Corpus oral, Entités nommées, Noms généraux, Noms de lieux, Lieux subjectifs, Subjectivité, Recettes de cuisine, Anonymisation des données, TAL, Humanities and Social Sciences/Linguistics, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art29" titre="Compact relaxations for polynomial programming problems">
<auteurs>CAFIERI Sonia, HANSEN Pierre, LÉTOCART Lucas, et al.</auteurs>
<titre_de_la_source>SEA 2012, 11th International Symposium on Experimental Algorithms</titre_de_la_source>
<date_de_publication>2012</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal-enac.archives-ouvertes.fr/hal-00938524</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112433P, 200519331V</numero_national_de_structure_de_recherche>
<doi>10.1007/978-3-642-30850-5_8</doi>
<reference_hal>hal-00938524</reference_hal>
<references_archives_oai>oai:HAL:hal-00938524v1</references_archives_oai>
</informations_complementaires>
<resume>Reduced RLT constraints are a special class of Reformulation- Linearization Technique (RLT) constraints. They apply to nonconvex (both continuous and mixed-integer) quadratic programming problems subject to systems of linear equality constraints. We present an extension to the general case of polynomial programming problems and discuss the derived convex relaxation. We then show how to perform rRLT constraint generation so as to reduce the number of inequality constraints in the relaxation, thereby making it more compact and faster to solve. We present some computational results validating our approach.</resume>
<thematiques>polynomial, nonconvex, MINLP, sBB, reformulation, convex relaxation, RLT, Mathematics [math]/Optimization and Control [math.OC]</thematiques>
</article>
<article id="art30" titre="Advances in Artificial Intelligence">
<auteurs>KÉGL B., LAPALME(EDS.) G.</auteurs>
<titre_de_la_source>18th Conference of the Canadian Society for Computational Studies of Intelligence</titre_de_la_source>
<date_de_publication>2005</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>http://hal.in2p3.fr/in2p3-00935523</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>199812934X, 199812948M</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>in2p3-00935523</reference_hal>
<references_archives_oai>oai:HAL:in2p3-00935523v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>AI logics, NLP, artificial intelligence, classification, clustering, constraint logic programming, data analysis, formal reasoning, intelligent agents, knowledge representation, learning theory, machine learning, multi, agent systems, natural language processing, Computer Science [cs]/Artificial Intelligence [cs.AI], Statistics [stat]/Machine Learning [stat.ML]</thematiques>
</article>
<article id="art31" titre="Post-Retrieval Clustering Using Third-Order Similarity Measures">
<auteurs>MORENO JoseG, DIAS Gaël, CLEUZIOU Guillaume</auteurs>
<titre_de_la_source>Annual Meeting of the Association for Computational Linguistics (ACL 2013)</titre_de_la_source>
<date_de_publication>2013</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00931263</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200012161Y, 200615274F</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00931263</reference_hal>
<references_archives_oai>oai:HAL:hal-00931263v1</references_archives_oai>
</informations_complementaires>
<resume>Post-retrieval clustering is the task of clustering Web search results. Within this context, we propose a new methodology that adapts the classical K-means algorithm to a third-order similarity measure initially developed for NLP tasks. Results obtained with the definition of a new stopping criterion over the ODP-239 and the MORESQUE golden standard datasets evidence that our proposal outperforms all reported text-based approaches.</resume>
<thematiques>Computer Science [cs]/Information Retrieval [cs.IR], Computer Science [cs]/Machine Learning [cs.LG]</thematiques>
</article>
<article id="art32" titre="Towards Context-Based Subjectivity Analysis">
<auteurs>BENAMARA Farah, CHARDON Baptiste, MATHIEU YvetteYannick, et al.</auteurs>
<titre_de_la_source>The 5th International Joint Conference on Natural Language Processing IJCNLP 2011</titre_de_la_source>
<date_de_publication>2011</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://halshs.archives-ouvertes.fr/halshs-00751064</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112497J</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>halshs-00751064</reference_hal>
<references_archives_oai>oai:HAL:halshs-00751064v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>C-ACTI</thematiques>
</article>
<article id="art33" titre="Prospects for Language Resource Standardisation">
<auteurs>ROMARY Laurent</auteurs>
<titre_de_la_source>6th Natural Language Processing Pacific Rim Symposium (NLPRS 2001), Post-Conference Workshop on Language Resources in Asia, National Center of Sciences</titre_de_la_source>
<date_de_publication>2001</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.inria.fr/inria-00525389</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>198912571S</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>inria-00525389</reference_hal>
<references_archives_oai>oai:HAL:inria-00525389v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art34" titre="MINLP Interface Specification">
<auteurs>PANTELIDES C.C., LIBERTI L., TSIAKIS P., et al.</auteurs>
<titre_de_la_source>CAPE-OPEN Update</titre_de_la_source>
<date_de_publication>2002</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00163564</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>200519331V</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00163564</reference_hal>
<references_archives_oai>oai:HAL:hal-00163564v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>Computer Science [cs]/Other [cs.OH]</thematiques>
</article>
<article id="art35" titre="Nuclear retention of the transcription factor NLP7 orchestrates the early response to nitrate in plants">
<auteurs>MARCHIVE Chloé, ROUDIER Francois, CASTAINGS Loren, et al.</auteurs>
<titre_de_la_source>Nature Communications</titre_de_la_source>
<date_de_publication>2013</date_de_publication>
<numero></numero>
<pagination>1713</pagination>
<lien>http://dx.doi.org/10.1038/ncomms2650</lien>
<issn>2041-1723</issn>
<informations_complementaires>
<type_de_publication>&article;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>201119640E</numero_national_de_structure_de_recherche>
<doi>10.1038/ncomms2650</doi>
<reference_hal>hal-01190574</reference_hal>
<references_archives_oai>oai:prodinra.inra.fr:208971, oai:HAL:hal-01190574v1</references_archives_oai>
</informations_complementaires>
<resume>Nitrate is both an important nutrient and a signalling molecule for plants. Although several components of the nitrate signalling pathway have been identified, their hierarchical organization remains unclear. Here we show that the localization of NLP7, a member of the RWP-RK transcription factor family, is regulated by nitrate via a nuclear retention mechanism. Genome-wide analyses revealed that NLP7 binds and modulates a majority of known nitrate signalling and assimilation genes. Our findings indicate that plants, like fungi and mammals, rely on similar nuclear retention mechanisms to instantaneously respond to the availability of key nutrients.</resume>
<thematiques>EXPORT RECEPTOR, GENE-EXPRESSION, ARABIDOPSIS, NITROGEN, METABOLISM, PROTEINS, IDENTIFICATION, REVEALS, SIGNALS, DEFINES</thematiques>
</article>
<article id="art36" titre="PCFG Induction for Unsupervised Parsing and Language Modelling">
<auteurs>SCICLUNA James, DELAHIGUERA Colin</auteurs>
<titre_de_la_source>Conference on Empirical Methods in Natural  Language Processing EMNLP</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01087537</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200812282V</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01087537</reference_hal>
<references_archives_oai>oai:HAL:hal-01087537v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art37" titre="Parallel areas detection in multi-documents for multilingual alignment">
<auteurs>LECLUZE Charlotte, BRIXTEL Romain, RIGOUSTE Loïs, et al.</auteurs>
<titre_de_la_source>20ème conférence du Traitement Automatique du Langage Naturel 2013 (TALN 2013)</titre_de_la_source>
<date_de_publication>2013</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01074950</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200012161Y</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01074950</reference_hal>
<references_archives_oai>oai:HAL:hal-01074950v1</references_archives_oai>
</informations_complementaires>
<resume>This article broaches a central issue of the automatic alignment : diagnosing the parallelism ofdocuments. Previous research was concentrated on the analysis of documents which are parallelby nature such as corpus of regulations, technical documents or simple sentences. Inversions anddeletions/additions phenomena that may exist between different versions of a document hasoften been overlooked. To the contrary, we propose a method to diagnose in context the parallelareas allowing the detection of deletions or inversions between documents to align. This originalmethod is based on the freeing from word and sentence as well as the consideration of the textformatting. The implementation is based on the detection of repeated character strings and theidentification of parallel segments by image processing.</resume>
<thematiques>area detection and alignment, character N-grams matching, multidocuments corpora, corpus de multidocuments, appariement de N-grammes de caractères, détection et alignement de zones, Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art38" titre="Ontology-based Technical Text Annotation">
<auteurs>LÉVY François, TOMEH Nadi, MA Yue</auteurs>
<titre_de_la_source>COLING Workshop on Synchronic and Diachronic Approaches to Analyzing Technical Language (SADAATL)</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01129988</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112433P</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01129988</reference_hal>
<references_archives_oai>oai:HAL:hal-01129988v1</references_archives_oai>
</informations_complementaires>
<resume>Powerful tools could help users explore and maintain domain specific documentations, provided that documents have been semantically annotated. For that, the annotations must be sufficiently specialized and rich, relying on some explicit semantic model, usually an ontology, that repre- sents the semantics of the target domain. In this paper, we learn to annotate biomedical scientific publications with respect to a Gene Regulation Ontology. We devise a two-step approach to an- notate semantic events and relations. The first step is recast as a text segmentation and labeling problem and solved using machine translation tools and a CRF, the second as multi-class classi- fication. We evaluate the approach on the BioNLP-GRO benchmark, achieving an average 61% F-measure on the event detection by itself and 50% F-measure on biological relation annotation. This suggests that human annotators can be supported in domain specific semantic annotation tasks. Under different experimental settings, we also conclude some interesting observations: (1) For event detection and compared to classical time-consuming sequence labeling approach, the newly proposed machine translation based method performed equally well but with much less computation resource required. (2) A highly domain specific part of the task, namely proteins and transcription factors detection, is best performed by domain aware tools, which can be used separately as an initial step of the pipeline.</resume>
<thematiques>Natural Language understanding - Semantic annotations - Machine Learning, Computer Science [cs]/Artificial Intelligence [cs.AI]</thematiques>
</article>
<article id="art39" titre="Using text to build semantic networks for pharmacogenomics">
<auteurs>COULET Adrien, SHAH NigamH, GARTEN Yael, et al.</auteurs>
<titre_de_la_source>Journal of Biomedical Informatics</titre_de_la_source>
<date_de_publication>2010</date_de_publication>
<numero>6</numero>
<pagination></pagination>
<lien>https://hal.inria.fr/inria-00549695</lien>
<issn>1532-0464</issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>198912571S</numero_national_de_structure_de_recherche>
<doi>10.1016/j.jbi.2010.08.005</doi>
<reference_hal>inria-00549695</reference_hal>
<references_archives_oai>oai:HAL:inria-00549695v1</references_archives_oai>
</informations_complementaires>
<resume>Most pharmacogenomics knowledge is contained in the text of published studies, and is thus not available for automated computation. Natural Language Processing (NLP) techniques for extracting relationships in specific domains often rely on hand-built rules and domain-specific ontologies to achieve good performance. In a new and evolving field such as pharmacogenomics (PGx), rules and ontologies may not be available. Recent progress in syntactic NLP parsing in the context of a large corpus of pharmacogenomics text provides new opportunities for automated relationship extraction. We describe an ontology of PGx relationships built starting from a lexicon of key pharmacogenomic entities and a syntactic parse of more than 87 million sentences from 17 million MEDLINE abstracts. We used the syntactic structure of PGx statements to systematically extract commonly occurring relationships and to map them to a common schema. Our extracted relationships have a 70-87.7% precision and involve not only key PGx entities such as genes, drugs, and phenotypes (e.g., VKORC1, warfarin, clotting disorder), but also critical entities that are frequently modified by these key entities (e.g., VKORC1 polymorphism, warfarin response, clotting disorder treatment). The result of our analysis is a network of 40,000 relationships between more than 200 entity types with clear semantics. This network is used to guide the curation of PGx knowledge and provide a computable resource for knowledge discovery.</resume>
<thematiques>Computer Science [cs]/Databases [cs.DB], Computer Science [cs]/Bioinformatics [q-bio.QM], Life Sciences [q-bio]/Quantitative Methods [q-bio.QM], Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art40" titre="Cross-Lingual Semantic Similarity Measure for Comparable Articles">
<auteurs>SAAD Motaz, LANGLOIS David, SMAÏLI Kamel</auteurs>
<titre_de_la_source>Advances in Natural Language Processing - 9th International Conference on NLP, PolTAL 2014, Warsaw, Poland, September 17-19, 2014. Proceedings</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.inria.fr/hal-01067687</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>198912571S</numero_national_de_structure_de_recherche>
<doi>10.1007/978-3-319-10888-9_11</doi>
<reference_hal>hal-01067687</reference_hal>
<references_archives_oai>oai:HAL:hal-01067687v1</references_archives_oai>
</informations_complementaires>
<resume>A measure of similarity is required to find and compare cross-lingual articles concerning a specific topic. This measure can be based on bilingual dictionaries or based on numerical methods such as Latent Semantic Indexing (LSI). In this paper, we use LSI in two ways to retrieve Arabic-English comparable articles. The first way is monolingual: the English article is translated into Arabic and then mapped into the Arabic LSI space,  the second way is cross-lingual: Arabic and English documents are mapped into Arabic-English LSI space. Then we compare LSI approaches to the dictionary-based approach on several English-Arabic parallel and comparable corpora. Results indicate that the performance of our cross-lingual LSI approach is competitive to the monolingual approach and even better for some corpora. Moreover, both LSI approaches outperform the dictionary approach.</resume>
<thematiques>Cross-lingual latent semantic indexing, corpus comparability, cross-lingual information retrieval, Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art41" titre="Modular resource development and diagnostic evaluation framework for fast NLP system improvement">
<auteurs>DECHALENDAR Gaël, NOUVEL Damien</auteurs>
<titre_de_la_source>North American Chapter of the Association for Computational Linguistics - Human Language Technologies 2009</titre_de_la_source>
<date_de_publication>2009</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00568775</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>201220254T</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00568775</reference_hal>
<references_archives_oai>oai:HAL:hal-00568775v1</references_archives_oai>
</informations_complementaires>
<resume>Natural Language Processing systems are large-scale softwares, whose development involves many man-years of work, in terms of both coding and resource development. Given a dictionary of 110k lemmas, a few hundred syntactic analysis rules, 20k ngrams matrices and other resources, what will be the impact on a syntactic analyzer of adding a new possible category to a given verb? What will be the consequences of a new syntactic rules addition? Any modification may imply, besides what was expected, unforeseeable side-effects and the complexity of the system makes it difficult to guess the overall impact of even small changes. We present here a framework designed to effectively and iteratively improve the accuracy of our linguistic analyzer LIMA by iterative refinements of its linguistic resources. These improvements are continuously assessed by evaluating the analyzer performance against a reference corpus. Our first results show that this framework is really helpful towards this goal.</resume>
<thematiques>Evaluation parsing, syntax, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art42" titre="Nothing like Good Old Frequency: Studying Context Filters for Distributional Thesauri">
<auteurs>PADRÓ Muntsa, IDIART Marco, VILLAVICENCIO Aline, et al.</auteurs>
<titre_de_la_source>Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2014) - short papers</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01200592</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>201220259Y</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01200592</reference_hal>
<references_archives_oai>oai:HAL:hal-01200592v1</references_archives_oai>
</informations_complementaires>
<resume>no abstract</resume>
<thematiques>Computer Science [cs]</thematiques>
</article>
<article id="art43" titre="Why Microsoft Arabic Spell checker is ineffective">
<auteurs>NEME AlexisAmid</auteurs>
<titre_de_la_source>Linguistica Communicatio</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01081965</lien>
<issn>0851-6774</issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>200212717U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01081965</reference_hal>
<references_archives_oai>oai:HAL:hal-01081965v1</references_archives_oai>
</informations_complementaires>
<resume>Since 1997, the MS Arabic spell checker was integrated by Coltec-Egypt in the MS-Office suite and till now many Arabic users find it worthless. In this study, we show why the MS-spell checker fails to attract Arabic users. After spell-checking a document (10 pages -3300 words in Arabic), the assessment procedure spots 78 false positive errors. They reveal the lexical resource flaws: an unsystematic lexical coverage of the feminine and the broken plural of nouns and adjectives, and an arbitrary coverage of verbs and nouns with prefixed or suffixed particles. This unsystematic and arbitrary lexical coverage of the language resources pinpoints the absence of a clear definition of a lexical entry and an inadequate design of the related agglutination rules. Finally, this assessment reveals in general the failure of scientific and technological policies in big companies and in research institutions regarding Arabic.</resume>
<thematiques>Computational Linguistics, Arabic, NLP, spelling error detection, dictionary, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art44" titre="Using shallow linguistic features for relation extraction in bio-medical texts">
<auteurs>EBADAT Ali-Reza, CLAVEAU Vincent, SÉBILLOT Pascale</auteurs>
<titre_de_la_source>Traitement Automatique des Langues Naturelles, TALN</titre_de_la_source>
<date_de_publication>2011</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00644070</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200012163A</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00644070</reference_hal>
<references_archives_oai>oai:HAL:hal-00644070v1</references_archives_oai>
</informations_complementaires>
<resume>In this paper2 , we model the corpus-based relation extraction task as a classification problem. We show that, in this framework, standard machine learning systems exploiting representations simply based on shallow linguistic information can rival state-of-the-art systems that rely on deep linguistic analysis. Even more effective systems can be obtained, still using these easy and reliable pieces of information, if the specifics of the extraction task and the data are taken into account. Our original method combining lazy learning and language modeling out-performs the existing systems when evaluated on the LLL2005 protein-protein interaction extraction task data.</resume>
<thematiques>Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art45" titre="A Serious Game for Second Language Acquisition">
<auteurs>AMOIA Marilisa, GARDENT Claire, PEREZ-BELTRACHINI Laura</auteurs>
<titre_de_la_source>Third International Conference on Computer Supported Education - CSEDU 2011</titre_de_la_source>
<date_de_publication>2011</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.inria.fr/hal-00643955</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>198912571S</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00643955</reference_hal>
<references_archives_oai>oai:HAL:hal-00643955v1</references_archives_oai>
</informations_complementaires>
<resume>This paper describes an interactive learning system specifically designed for second language acquisition. In order to render the learning experience more fun, to engage the learner and to help him maintaining long-term motivation, the system was implemented as a 3D video game. It brings together the ability of virtual reality environments such as Second Life to reproduce immersive experiences and NLP language technology, thereby provided both situated learning and and automatic authoring of training activities in context</resume>
<thematiques>Virtual Learning Environments, Computer Assisted Learning, Intelligent Tutoring Systems, Immersive Learning, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art46" titre="Classifieur probabiliste avec Support Vector Machine (SVMs) et Okapi">
<auteurs>TRINH AnhPhuc, BUFFONI David, GALLINARI Patrick</auteurs>
<titre_de_la_source>Conference TALN08</titre_de_la_source>
<date_de_publication>2008</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01301622</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>199712651U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01301622</reference_hal>
<references_archives_oai>oai:HAL:hal-01301622v1</references_archives_oai>
</informations_complementaires>
<resume>Ce papier présente le travail réalisé par l’équipe des jeunes chercheurs du LIP6 pour le 4ème DÉfi Fouille de Textes (DEFT’08). Cette année, le défi était de classifier les documents de 2 corpus différents en prenant en compte les variations en genre et en thème. Cet article présente un modèle de classification automatique sous la forme de SVMs estimant les probabilités a posteriori des classes pour chaque document.</resume>
<thematiques>Computer Science [cs]</thematiques>
</article>
<article id="art47" titre="Tree analogical learning. Application in NLP">
<auteurs>BENHASSENA Anouar, MICLET Laurent</auteurs>
<titre_de_la_source>Traitement Automatique des Langues Naturelles</titre_de_la_source>
<date_de_publication>2010</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.inria.fr/inria-00594185</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200012163A</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>inria-00594185</reference_hal>
<references_archives_oai>oai:HAL:inria-00594185v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>Computer Science [cs]/Artificial Intelligence [cs.AI], Computer Science [cs]/Machine Learning [cs.LG]</thematiques>
</article>
<article id="art48" titre="Discriminative Training Procedure for Continuous-Space Translation Models">
<auteurs>DO Quockhanh</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2016</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://tel.archives-ouvertes.fr/tel-01315755</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source></type_de_la_source>
<numero_national_de_structure_de_recherche>197217542U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>tel-01315755</reference_hal>
<references_archives_oai>oai:HAL:tel-01315755v1</references_archives_oai>
</informations_complementaires>
<resume>Over the past few years, neural network (NN) architectures have been successfully applied to many Natural Language Processing (NLP) applications, such as Automatic Speech Recognition (ASR) and Statistical Machine Translation (SMT).For the language modeling task, these models consider linguistic units (i.e words and phrases) through their projections into a continuous (multi-dimensional) space, and the estimated distribution is a function of these projections. Also qualified continuous-space models (CSMs), their peculiarity hence lies in this exploitation of a continuous representation that can be seen as an attempt to address the sparsity issue of the conventional discrete models. In the context of SMT, these echniques have been applied on neural network-based language models (NNLMs) included in SMT systems, and oncontinuous-space translation models (CSTMs). These models have led to significant and consistent gains in the SMT performance, but are also considered as very expensive in training and inference, especially for systems involving large vocabularies. To overcome this issue, Structured Output Layer (SOUL) and Noise Contrastive Estimation (NCE) have been proposed,  the former modifies the standard structure on vocabulary words, while the latter approximates the maximum-likelihood estimation (MLE) by a sampling method. All these approaches share the same estimation criterion which is the MLE ,  however using this procedure results in an inconsistency between theobjective function defined for parameter  stimation and the way models are used in the SMT application. The work presented in this dissertation aims to design new performance-oriented and global training procedures for CSMs to overcome these issues. The main contributions lie in the investigation and evaluation of efficient training methods for (large-vocabulary) CSMs which aim~:(a) to reduce the total training cost, and (b) to improve the efficiency of these models when used within the SMT application. On the one hand, the training and inference cost can be reduced (using the SOUL structure or the NCE algorithm), or by reducing the number of iterations via a faster convergence. This thesis provides an empirical analysis of these solutions on different large-scale SMT tasks. On the other hand, we propose a discriminative training framework which optimizes the performance of the whole system containing the CSM as a component model. The experimental results show that this framework is efficient to both train and adapt CSM within SMT systems, opening promising research perspectives.</resume>
<thematiques>Statistical Machine Translation, Neural Network, Continuous-Space Models, Discriminative Training, Large-Margin Methods, Noise Contrastive Estimation, Réseau de neurones, Modèles Continus de Traduction, Apprentissage Discriminant, Méthodes à Larges Marges, Estimation Contrastive Bruitée, Traduction Automatique Statistique, Computer Science [cs]/Machine Learning [cs.LG], Statistics [stat]/Machine Learning [stat.ML], Mathematics [math]/Statistics [math.ST]</thematiques>
</article>
<article id="art49" titre="Looking for French deverbal nouns in an evolving Web (a short history of WAC)">
<auteurs>HATHOUT Nabil, SAJOUS Franck, TANGUY Ludovic</auteurs>
<titre_de_la_source>Fifth Workshop on Web As Corpus</titre_de_la_source>
<date_de_publication>2009</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://halshs.archives-ouvertes.fr/halshs-00414494</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200711926M</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>halshs-00414494</reference_hal>
<references_archives_oai>oai:HAL:halshs-00414494v1</references_archives_oai>
</informations_complementaires>
<resume>This paper describes an 8-year-long research effort for automatically collecting new French deverbal nouns on the Web. The goal has remained the same: building an extensive and cumulative list of noun-verb pairs where the noun denotes the action expressed by the verb (e.g. production - produce). This list is used for both linguistic research and for NLP applications. The initial method consisted in taking advantage of the former Altavista search engine, allowing for a direct access to unknown word forms. The second technique led us to develop a specific crawler, which raised a number of technical difficulties. In the third experiment, we use a collection of web pages made available to us by a commercial search engine. Through all these stages, the general method has remained the same, and the results are similar and cumulative, although the technical environment has greatly evolved.</resume>
<thematiques>Web, Corpus, Deverbal nouns, morphology, Lexicon, Humanities and Social Sciences/Linguistics, Computer Science [cs]/Computation and Language [cs.CL], Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art50" titre="Le projet LiSe : LInguistique, normes, traitement automatique des langues et SEcurité : du « data et sense mining » aux langues contrôlées">
<auteurs>RENAHY J., THOMAS I.</auteurs>
<titre_de_la_source>Conférence JEP/TALN 2008</titre_de_la_source>
<date_de_publication>2008</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00491546</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>199613838N</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00491546</reference_hal>
<references_archives_oai>oai:HAL:hal-00491546v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>Lise, projet LiSe, Linguistique normes, traitement automatique des langues et Sécurité, data mining, sense mining, langues contrôlées, Linguistique et Sécurité, Humanities and Social Sciences/Linguistics, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art51" titre="WordNet et son écosystème : un ensemble de ressources linguistiques de large couverture">
<auteurs>CHAUMARTIN François-Régis</auteurs>
<titre_de_la_source>Colloque BD lexicales</titre_de_la_source>
<date_de_publication>2007</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00611240</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200012753S</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00611240</reference_hal>
<references_archives_oai>oai:HAL:hal-00611240v1</references_archives_oai>
</informations_complementaires>
<resume>Everybody knows WordNet, but do you know everything inside it? We propose here to rediscover WordNet (especially with the latest versions) and other linked (lexical, syntactic and semantic) resources. We also present techniques for automatically extend WordNet, and NLP applications using it.</resume>
<thematiques>semantic Web, ontologies, WordNet, eXtended WordNet, VerbNet, FrameNet, SentiWordNet, WordNet Domains, WordNet-Affect, SemCor, Wikipédia, SUMO, Cyc, Web sémantique, ontologie, Humanities and Social Sciences/Linguistics, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art52" titre="Final report on NLP analysis and normalization">
<auteurs>NAZARENKO Adeline, AUBIN Sophie, BOSSY Robert, et al.</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2007</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00711871</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source></type_de_la_source>
<numero_national_de_structure_de_recherche>200112433P</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00711871</reference_hal>
<references_archives_oai>oai:HAL:hal-00711871v1</references_archives_oai>
</informations_complementaires>
<resume>This document describes the ALVIS NLP line that has been developed. In ALVIS, it is used to analyze the crawled documents that are then indexed on semantic and domain-specific grounds. It is also to process training corpora in order to acquire the specialized linguistic and domain resources (in WP6) that are then exploited for the semantic analysis of larger document collections. Four languages are addressed in ALVIS: English, French, Slovene and Chinese. Some specific NLP modules have been developed and integrated for each language. The resulting NLP lines have been tested trough several experiments on different domains, on document collections of various size and with various degrees of NLP analysis.</resume>
<thematiques>Natural Language Processing, robustness, sub-language processing, Chinese processing, Slovene processing, Biological Natural Language Processing, Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art53" titre="Biomedical Event Extraction by Multi-class Classification of Pairs of Text Entities">
<auteurs>LIU Xiao, BORDES Antoine, GRANDVALET Yves</auteurs>
<titre_de_la_source>BioNLP Shared Task 2013 Workshop</titre_de_la_source>
<date_de_publication>2013</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00880444</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>201220263C</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00880444</reference_hal>
<references_archives_oai>oai:HAL:hal-00880444v1</references_archives_oai>
</informations_complementaires>
<resume>This paper describes the HDS4NLP en- try to the BioNLP 2013 shared task on biomedical event extraction. This system is based on a pairwise model that transforms trigger classification in a simple multi-class problem in place of the usual multi-label problem. This model facilitates inference compared to global models while relying on richer information compared to usual pipeline approaches. The HDS4NLP system ranked 6th on the Genia task (43.03% f-score), and after fixing a bug discovered after the final submission, it outperforms the winner of this task (with a f-score of 51.15%).</resume>
<thematiques>Computer Science [cs]/Machine Learning [cs.LG]</thematiques>
</article>
<article id="art54" titre="Interesting Linguistic Features in Coreference Annotation of an Inflectional Language">
<auteurs>OGRODNICZUK Maciej, KATARZYNA Glowinska, MATEUSZ Kopec, et al.</auteurs>
<titre_de_la_source>The 12th China National Conference, CCL 2013 and First International Symposium, NLP-NABD 2013</titre_de_la_source>
<date_de_publication>2013</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01215982</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>201220254T</numero_national_de_structure_de_recherche>
<doi>10.1007/978-3-642-41491-6_10</doi>
<reference_hal>hal-01215982</reference_hal>
<references_archives_oai>oai:HAL:hal-01215982v1</references_archives_oai>
</informations_complementaires>
<resume>This paper reports on linguistic features and decisions that we findvital in the process of annotation and resolution of coreference for highly inflec-tional languages. The presented results have been collected during preparation ofa corpus of general direct nominal coreference of Polish. Starting from the notionof a mention, its borders and potential vs. actual referentiality, we discuss theproblem of complete and near-identity, zero subjects and dominant expressions.We also present interesting linguistic cases influencing the coreference resolutionsuch as the difference between semantic and syntactic heads or the phenomenonof coreference chains made of indefinite pronouns.</resume>
<thematiques>Computer Science [cs]/Computation and Language [cs.CL], Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art55" titre="An Earley Parsing Algorithm for Range Concatenation Grammars">
<auteurs>KALLMEYER Laura, MAIER Wolfgang, PARMENTIER Yannick</auteurs>
<titre_de_la_source>Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP 2009)</titre_de_la_source>
<date_de_publication>2009</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.inria.fr/inria-00393980</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>198912571S</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>inria-00393980</reference_hal>
<references_archives_oai>oai:HAL:inria-00393980v1</references_archives_oai>
</informations_complementaires>
<resume>We present a CYK and an Earley-style algorithm for parsing Range Concatenation Grammar (RCG), using the deductive parsing framework. The characteristic property of the Earley parser is that we use a technique of range boundary constraint propagation to compute the yields of non-terminals as late as possible. Experiments show that, compared to previous approaches, the constraint propagation helps to considerably decrease the number of items in the chart.</resume>
<thematiques>Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art56" titre="Enhancing Hyperspectral Image Quality using Nonlinear PCA">
<auteurs>LICCIARDI Giorgio, CHANUSSOT Jocelyn, VASILE Gabriel, et al.</auteurs>
<titre_de_la_source>IEEE International Conference on Image Processing</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01065843</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200711885T</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01065843</reference_hal>
<references_archives_oai>oai:HAL:hal-01065843v1</references_archives_oai>
</informations_complementaires>
<resume>In this paper, we propose a new method aiming at reducing the noise in hyperspectral images. It is based on the nonlinear generalization of Principal Component Analysis (NLPCA). The NLPCA is performed by an auto associative neural network that have the hyperspectral image as input and is trained to reconstruct the same image at the output. Thanks to its bottleneck structure, the AANN forces the hyper spectral image to be projected in a lower dimensionality feature space where noise as well as both linear and nonlinear correlations between spectral bands are removed. This process permits to obtain enhancements in terms of hyperspectral image quality. Experiments are conducted on different real hyper spectral images, with different contexts and resolutions. The results are qualitatively and quantitatively discussed and demonstrate the interest of the proposed method as compared to traditional approaches.</resume>
<thematiques>Engineering Sciences [physics]/Signal and Image processing, Computer Science [cs]/Signal and Image Processing</thematiques>
</article>
<article id="art57" titre="ZOMBILINGO : manger des têtes pour annoter en syntaxe de dépendances">
<auteurs>FORT Karën, GUILLAUME Bruno, STERN Valentin</auteurs>
<titre_de_la_source>TALN - Traitement Automatique des Langues Naturelles</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.inria.fr/hal-01054395</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>198912571S</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01054395</reference_hal>
<references_archives_oai>oai:HAL:hal-01054395v1</references_archives_oai>
</informations_complementaires>
<resume>This paper presents ZOMBILINGO, a Game With A Purpose (GWAP) that allows for the dependency syntax annotation of French corpora. The created resource is freely available on the game Web site.</resume>
<thematiques>annotation, dependency syntax, complexity, GWAP, Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art58" titre="Sensor Fault Detection and Isolation of an Air Quality Monitoring Network Using Nonlinear Principal component Analysis">
<auteurs>HARKAT Mohamed-Faouzi, RAGOT José, MOUROT Gilles</auteurs>
<titre_de_la_source>IFAC World Congress</titre_de_la_source>
<date_de_publication>2005</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00511729</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112440X</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00511729</reference_hal>
<references_archives_oai>oai:HAL:hal-00511729v1</references_archives_oai>
</informations_complementaires>
<resume>Recently, fault detection and process monitoring using principal component analysis (PCA) were studied intensively and largely applied to industrial process. PCA is the optimal linear transformation with respect to minimizing the mean squared prediction error. If the data have nonlinear dependencies, an important issue is to develop a technique which can take into account this kind of dependencies. Recognizing the shortcomings of PCA, a nonlinear extension of PCA is developed. This paper proposes an application for sensor failure detection and isolation (FDI) to an air quality monitoring network via nonlinear principal component analysis (NLPCA). The NLPCA model is obtained by using two cascade three layer RBF-Networks. For training these two networks separately, the outputs of the first network are estimated using principal curve algorithm [7] and the problem is transformed as two nonlinear regression problems.</resume>
<thematiques>air monitoring network, sensor fault detection, model-based fault diagnosis, principal curves, nonlinear PCA, radial basis functions, air pollution, air monitoring network., Computer Science [cs]/Automatic Control Engineering</thematiques>
</article>
<article id="art59" titre="Standardization of the formal representation of lexical information for NLP">
<auteurs>ROMARY Laurent</auteurs>
<titre_de_la_source>Dictionaries. An International Encyclopedia of Lexicography. Supplementary volume: Recent developments with special focus on computational lexicography</titre_de_la_source>
<date_de_publication>2013</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00436328</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Ouvrage</type_de_la_source>
<numero_national_de_structure_de_recherche>199812948M</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00436328</reference_hal>
<references_archives_oai>oai:HAL:hal-00436328v1</references_archives_oai>
</informations_complementaires>
<resume>A survey of dictionary models and formats is presented as well as a presentation of corresponding recent standardisation activities.</resume>
<thematiques>Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art60" titre="Mutualisation et uniformisation de ressources de français parlé">
<auteurs>BÉRARD Lolita, BENZITOUN Christophe</auteurs>
<titre_de_la_source>Les cahiers de praxématique</titre_de_la_source>
<date_de_publication>2010</date_de_publication>
<numero>54-55</numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01103605</lien>
<issn>0765-4944</issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>200112505T</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01103605</reference_hal>
<references_archives_oai>oai:HAL:hal-01103605v1</references_archives_oai>
</informations_complementaires>
<resume>According to the cost of speech transcription, it is very important to pool data to obtain a big size corpus to describe French. Our work consisted to pool six spoken French corpora, each with a specific goal (sociolinguistics, phonology, syntax), to format them for automatic exploitations. Indeed the next step will be to use NLP corpus tools (tagger, parser, concordancer). This experience showed that it is very important to specify recommendations for transcription conventions to make easier sharing and pooling data.</resume>
<thematiques>pooling, speech corpora, computation, mutualisation, corpus oraux, transcription, informatisation, Cognitive science/Linguistics</thematiques>
</article>
<article id="art61" titre="TextBox, a Written Corpus Tool for Linguistic Analysis">
<auteurs>CARTIER Emmanuel</auteurs>
<titre_de_la_source>Web As Corpus 2007</titre_de_la_source>
<date_de_publication>2007</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://halshs.archives-ouvertes.fr/halshs-00410977</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200712563E</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>halshs-00410977</reference_hal>
<references_archives_oai>oai:HAL:halshs-00410977v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>TALN, Fouille de texte, Corpus Web, Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art62" titre="Repérage automatique des expressions figées : état des lieux, perspectives">
<auteurs>CARTIER Emmanuel</auteurs>
<titre_de_la_source>Les séquences figées : entre langue et discours</titre_de_la_source>
<date_de_publication>2008</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://halshs.archives-ouvertes.fr/halshs-00410892</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Ouvrage</type_de_la_source>
<numero_national_de_structure_de_recherche>200712563E</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>halshs-00410892</reference_hal>
<references_archives_oai>oai:HAL:halshs-00410892v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>Expression figée, TALN, Extraction automatique, Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art63" titre="Chaînes de traitement syntaxique">
<auteurs>BOULLIER Pierre, CLÉMENT Lionel, SAGOT Benoît, et al.</auteurs>
<titre_de_la_source>TALN 05</titre_de_la_source>
<date_de_publication>2005</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00413183</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>199511665F</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00413183</reference_hal>
<references_archives_oai>oai:HAL:hal-00413183v1</references_archives_oai>
</informations_complementaires>
<resume>This paper presents a method in order to have a good syntactic representation of written texts.</resume>
<thematiques>Computer Science [cs]/Other [cs.OH]</thematiques>
</article>
<article id="art64" titre="Ontology Population via NLP Techniques in Risk Management">
<auteurs>MAKKI Jawad, ALQUIER Anne-Marie, PRINCE Violaine</auteurs>
<titre_de_la_source>ICSWE: Fifth International Conference on Semantic Web Engineering</titre_de_la_source>
<date_de_publication>2008</date_de_publication>
<numero>3</numero>
<pagination></pagination>
<lien>http://hal-lirmm.ccsd.cnrs.fr/lirmm-00332102</lien>
<issn>2220-8488</issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>199111950H</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>lirmm-00332102</reference_hal>
<references_archives_oai>oai:HAL:lirmm-00332102v1, oai:HAL:lirmm-00465555v1</references_archives_oai>
</informations_complementaires>
<resume>In this paper we propose an NLP-based method for Ontology Population from texts and apply it to semi automatic instantiate a Generic Knowledge Base (Generic Domain Ontology) in the risk management domain. The approach is semi-automatic and uses a domain expert intervention for validation. The proposed approach relies on a set of Instances Recognition Rules based on syntactic structures, and on the predicative power of verbs in the instantiation process. It is not domain dependent since it heavily relies on linguistic knowledge. A description of an experiment performed on a part of the ontology of the PRIMA project (supported by the European community) is given. A first validation of the method is done by populating this ontology with Chemical Fact Sheets from Environmental Protection Agency . The results of this experiment complete the paper and support the hypothesis that relying on the predicative power of verbs in the instantiation process improves the performance.</resume>
<thematiques>Information Extraction, Instance Recognition Rules, Ontology Population, Risk Management, Semantic Analysis, Computer Science [cs]/Document and Text Processing, Humanities and Social Sciences/Business administration, Computer Science [cs]/Web</thematiques>
</article>
<article id="art65" titre="Shallow Text Clustering Does Not Mean Weak Topics: How Topic Identification Can Leverage Bigram Features">
<auteurs>VELCIN Julien, ROCHE Mathieu, PONCELET Pascal</auteurs>
<titre_de_la_source>DMNLP: Data Mining and Natural Language Processing</titre_de_la_source>
<date_de_publication>2016</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>http://hal-lirmm.ccsd.cnrs.fr/lirmm-01362434</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>199914397H, 200718239Z, 199111950H</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>lirmm-01362434</reference_hal>
<references_archives_oai>oai:HAL:lirmm-01362434v1</references_archives_oai>
</informations_complementaires>
<resume>Text clustering and topic learning are two closely related tasks. In this paper, we show that the topics can be learnt without the absolute need of an exact categorization. In particular, the experiments performed on two real case studies with a vocabulary based on bigram features lead to extracting readable topics that cover most of the documents. Precision at 10 is up to 74% for a dataset of scientific abstracts with 10,000 features, which is 4% less than when using unigrams only but provides more interpretable topics.</resume>
<thematiques>Computer Science [cs]/Databases [cs.DB]</thematiques>
</article>
<article id="art66" titre="Confronter des ressources de connaissances différentes pour obtenir une réponse plus fiable">
<auteurs>DECHALENDAR Gaël, ELKATEB-GARA Faïza, FERRET Olivier, et al.</auteurs>
<titre_de_la_source>Dixi`eme conférence de Traitement Automatique des Langues Naturelles (TALN 2003)</titre_de_la_source>
<date_de_publication>2003</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00456517</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200812282V</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00456517</reference_hal>
<references_archives_oai>oai:HAL:hal-00456517v1</references_archives_oai>
</informations_complementaires>
<resume>no abstract</resume>
<thematiques>Computer Science [cs]/Document and Text Processing, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art67" titre="Some Interval Approximation Techniques for MINLP">
<auteurs>BERGER Nicolas, GRANVILLIERS Laurent</auteurs>
<titre_de_la_source>SARA 2009: The Eighth Symposium on Abstraction, Reformulation and Approximation</titre_de_la_source>
<date_de_publication>2009</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00462786</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200812282V</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00462786</reference_hal>
<references_archives_oai>oai:HAL:hal-00462786v1</references_archives_oai>
</informations_complementaires>
<resume>MINLP problems are hard constrained optimization problems, with nonlinear constraints and mixed discrete continuous variables. They can be solved using a Branch-and-Bound scheme combining several methods, such as linear programming, interval analysis, and cutting methods. Our goal is to integrate constraint programming techniques in this framework. Firstly, global constraints can be introduced to reformulate MINLP problems thus leading to clean models and more precise computations. Secondly, interval-based approximation techniques for nonlinear constraints can be improved by taking into account the integrality of variables early. These methods have been implemented in an interval solver and we present experimental results from a set of MINLP instances.</resume>
<thematiques>Computer Science [cs]/Artificial Intelligence [cs.AI]</thematiques>
</article>
<article id="art68" titre="Les tables du Lexique-Grammaire au format TAL">
<auteurs>TOLONE Elsa</auteurs>
<titre_de_la_source>MajecSTIC 2009</titre_de_la_source>
<date_de_publication>2009</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00461896</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200212717U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00461896</reference_hal>
<references_archives_oai>oai:HAL:hal-00461896v1</references_archives_oai>
</informations_complementaires>
<resume>Lexicon-Grammar tables are a very rich syntactic lexicon for the French language. This linguistic database is nevertheless not suitable for use by computer programs, as it is incomplete and lacks consistency. Our goal is to adapt the tables, so as to make them usable in various Natural Language Processing (NLP) applications. We describe the problems we encountered and the approaches we followed to enable their integration into a parser.</resume>
<thematiques>syntactic lexicon, Lexicon-Grammar, parsing, NLP, Computer Science [cs]/Computation and Language [cs.CL], Cognitive science/Linguistics, Cognitive science/Computer science</thematiques>
</article>
<article id="art69" titre="Do we still Need Gold Standards for Evaluation?">
<auteurs>POIBEAU Thierry, MESSIANT Cédric</auteurs>
<titre_de_la_source>Language Resource and Evaluation Conference</titre_de_la_source>
<date_de_publication>2008</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00321436</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112433P</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00321436</reference_hal>
<references_archives_oai>oai:HAL:hal-00321436v1</references_archives_oai>
</informations_complementaires>
<resume>The availability of a huge mass of textual data in electronic format has increased the need for fast and accurate techniques for textual data processing. Machine learning and statistical approaches have been increasingly used in NLP since the 1990s, mainly because they are quick, versatile and efficient. However, despite this evolution of the field, evaluation still rely (most of the time) on a comparison between the output of a probabilistic or statistical system on the one hand, and a non-statistic, most of the time hand-crafted, gold standard on the other hand. In order to be able to compare these two sets of data, which are inherently of a different nature, it is first necessary to modify the statistical data so that they fit with the hand-crafted reference. For example, a statistical parser, instead of producing a score of grammaticality, will have to produce a binary value for each sentence (grammatical vs ungrammatical) or a tree similar to the one stored in the treebank used as a reference. In this paper, we take the example of the acquisition of subcategorization frames from corpora as a practical example. Our study is motivated by the fact that, even if a gold standard is an invaluable resource for evaluation, a gold standard is always partial and does not really show how accurate and useful results are. We describe the task (SCF acquisition) and show how it is a typical NLP task. We then very briefly describe our SCF acquisition system before discussing different issues related to the evaluation using a gold standard. Lastly, we adopt the classical distinction between intrinsic and extrinsic evaluation and show why this framework is relevant for SCF acquisition. We show that, even if intrinsic evaluation correlates with extrinsic evaluation, these two evaluation frameworks give a complementary insight on the results. In the conclusion, we quickly discuss the case of other NLP tasks.</resume>
<thematiques>Computer Science [cs]/Artificial Intelligence [cs.AI], Humanities and Social Sciences/Linguistics, Cognitive science/Computer science, Cognitive science/Linguistics</thematiques>
</article>
<article id="art70" titre="French frozen verbal expressions: from lexicon-grammar tables to NLP applications.">
<auteurs>BENOIT Sagot, LAURENCE Danlos, SALMON-ALT Susanne</auteurs>
<titre_de_la_source>Colloque Lexique et Grammaire 2006</titre_de_la_source>
<date_de_publication>2006</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00110974</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200012753S, 200112505T</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00110974</reference_hal>
<references_archives_oai>oai:HAL:hal-00110974v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art71" titre="Parenthetical Classification for Information Extraction">
<auteurs>ELMAAROUF Ismaïl, VILLANEAU Jeanne</auteurs>
<titre_de_la_source>COLING 2012</titre_de_la_source>
<date_de_publication>2012</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00768590</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200012163A</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00768590</reference_hal>
<references_archives_oai>oai:HAL:hal-00768590v1</references_archives_oai>
</informations_complementaires>
<resume>The article focuses on a rather unexplored topic in NLP: parenthetical classification. Parentheticals are defined as any text sequence between parentheses. They have been approached from isolated perspectives, like translation pairs extraction, but a full account of their syntactic and semantic properties is lacking. This article proposes a new comprehensive scheme drawn from corpus-based linguistic studies on French news. This research is part of a project investigating the structural aspects of punctuation signs and their usefulness for Information Extraction. Parenthetical classification is approached as a relation extraction problem split into three correlated subtasks: syntactic and semantic classification and head recognition. Corpus-based studies singled out 11 syntactic and 18 semantic relation subtypes. The article addresses automatic classification, using a combination of CRF and SVM. This baseline system reports 0.674 (head recognition), 0.908 (syntax), 0.734 (semantics), and 0.518 (end-to-end) of F1.</resume>
<thematiques>Parentheticals, Punctuation, Information Extraction, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art72" titre="An Environment for the Joint Management of Written Policies and Business Rules">
<auteurs>LÉVY François, GUISSÉ Abdoulaye, NAZARENKO Adeline, et al.</auteurs>
<titre_de_la_source>2010 22nd IEEE International Conference on Tools with Artificial Intelligence</titre_de_la_source>
<date_de_publication>2010</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>http://dx.doi.org/10.1109/ictai.2010.95</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&communication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112433P</numero_national_de_structure_de_recherche>
<doi>10.1109/ictai.2010.95</doi>
<reference_hal>hal-00534509</reference_hal>
<references_archives_oai>oai:HAL:hal-00534509v1</references_archives_oai>
</informations_complementaires>
<resume>The contemporary world produces huge bodies of policies and regulations, while the underlying procedures tend to be automated in decision systems, which are designed to define, deploy, execute, monitor and maintain the various rules to which an organization or enterprise has to comply. It is important that the written documentation is integrated into such decision systems in order to refer to the texts to explain decisions, to update the systems when the policy evolves or, conversely, to amend the source documents if some of the rules happen to be inconsistent. The problem is that the complexity of information to be searched for is not reachable by an automated processing, but that their volume prohibits a manual one. Arguing that the integration of policies in decision systems can be better achieved through a semantic annotation than by the full parsing of the source documentation, this paper presents a technical environment that enables the building and exploitation of such semantic annotations.</resume>
<thematiques>TALN, Annotation, Index, Business Rules, Computer Science [cs]/Artificial Intelligence [cs.AI]</thematiques>
</article>
<article id="art73" titre="GLÀFF, a Large Versatile French Lexicon">
<auteurs>HATHOUT Nabil, SAJOUS Franck, CALDERONE Basilio</auteurs>
<titre_de_la_source>Conference on Language Resources and Evaluation (LREC)</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00998467</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200711926M</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00998467</reference_hal>
<references_archives_oai>oai:HAL:hal-00998467v1</references_archives_oai>
</informations_complementaires>
<resume>This paper introduces GLAFF, a large-scale versatile French lexicon extracted from Wiktionary, the collaborative online dictionary. GLAFF contains, for each entry, inflectional features and phonemic transcriptions. It distinguishes itself from the other available French lexicons by its size, its potential for constant updating and its copylefted license. We explain how we have built GLAFF and compare it to other known resources in terms of coverage and quality of the phonemic transcriptions. We show that its size and quality are strong assets that could allow GLAFF to become a reference lexicon for French NLP and linguistics. Moreover, other derived lexicons can easily be based on GLAFF to satisfy specific needs of various fields such as psycholinguistics.</resume>
<thematiques>Phonetic Databases, Phonology, Morphology, Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art74" titre="From Polysemy to Semantic Change: Towards a Typology of Lexical Semantic Associations">
<auteurs>VANHOVE Martine</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2008</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://halshs.archives-ouvertes.fr/halshs-00526104</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source></type_de_la_source>
<numero_national_de_structure_de_recherche>200212791Z</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>halshs-00526104</reference_hal>
<references_archives_oai>oai:HAL:halshs-00526104v1</references_archives_oai>
</informations_complementaires>
<resume>This book is the result of a joint project on lexical and semantic typology which gathered together field linguists, semanticists, cognitivists, typologists, and an NLP specialist. These cross-linguistic studies concern semantic shifts at large, both synchronic and diachronic: the outcome of polysemy, heterosemy, or semantic change at the lexical level. The first part presents a comprehensive state of the art of a domain typologists have long been reluctant to deal with. Part two focuses on theoretical and methodological approaches: cognition, construction grammar, graph theory, semantic maps, and data bases. These studies deal with universals and variation across languages, illustrated with numerous examples from different semantic domains and different languages. Part three is dedicated to detailed empirical studies of a large sample of languages in a limited set of semantic fields. It reveals possible universals of semantic association, as well as areal and cultural tendencies</resume>
<thematiques>Typology, semantics, cognition, construction grammar, lexical typology, semantic typology, polysemy, historical linguistics, theoretical linguistics, Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art75" titre="Modelling language resources - towards reference standards in NLP">
<auteurs>ROMARY Laurent</auteurs>
<titre_de_la_source>Seminar of the Natural Language and Information Processing Group, Computer Laboratory</titre_de_la_source>
<date_de_publication>2003</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.inria.fr/inria-00525742</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>198912571S</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>inria-00525742</reference_hal>
<references_archives_oai>oai:HAL:inria-00526955v1, oai:HAL:inria-00525742v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art76" titre="Lexique des affects : constitution de ressources pédagogiques numériques.">
<auteurs>AUGUSTYN Magdalena, BENHAMOU Sabrina, BLOQUET Gwendoline, et al.</auteurs>
<titre_de_la_source>Colloque International des étudiants-chercheurs en didactique des langues et linguistique.</titre_de_la_source>
<date_de_publication>2006</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://halshs.archives-ouvertes.fr/halshs-00418143</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>199113164C</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>halshs-00418143</reference_hal>
<references_archives_oai>oai:HAL:halshs-00418143v1</references_archives_oai>
</informations_complementaires>
<resume>The work presented in this article is part of a project aiming at the development and the NLP (Natural Language Processing) based exploitation of French teaching pedagogical resources. The article focuses on the “affect lexicon” to exemplify the process of computer modelling of the textual resources, the description of the lexicon and its semantic annotation within the corpus, on which is based the extraction of examples and the generation of exercises.</resume>
<thematiques>lexique des affects, polysémie, annotation, didactique, ALAO (apprentissage des langues assisté par ordinateur), Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art77" titre="The Role of a Linguistic Architecture in Language Processing and its Resources">
<auteurs>CAILLIAU Frederik</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2010</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://tel.archives-ouvertes.fr/tel-00546798</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source></type_de_la_source>
<numero_national_de_structure_de_recherche>200112433P</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>tel-00546798</reference_hal>
<references_archives_oai>oai:HAL:tel-00546798v1</references_archives_oai>
</informations_complementaires>
<resume>Systems integrating natural language processing often use lexicons and grammars, sometimes indirectly corpora. Because of the quantity and the complexity of the information in these linguistic resources, they are likely to become a source of inconsistency. In this thesis we explore how to improve the management of linguistic resources for an industrial search engine in nineteen languages that performs an elaborate textual analysis. We propose a method to formalize the linguistic architecture of the linguistic processing and its resources. This formalization shows how the knowledge contained in the resources is exploited and gives us the possibility to build management tools compliant with the system‘s architecture. The environment implemented in this way focuses on updating and acquiring the linguistic resources, while their exploitation is defined by the industrial constraints.</resume>
<thematiques>linguistic architecture, linguistic resource, linguistic resource management, NLP system, NLP tool, natural language processing, architecture linguistique, ressource linguistique, gestion de ressources linguistiques, système de TAL, traitement automatique des langues, Computer Science [cs]/Human-Computer Interaction [cs.HC]</thematiques>
</article>
<article id="art78" titre="Robustesse et portabilités multilingue et multi-domaines des systèmes de compréhension de la parole : les corpus du projet PortMedia (Robustness and portability of spoken language understanding systems among languages and domains : the PORTMEDIA project)">
<auteurs>LEFEVRE Fabrice, MOSTEFA Djamel, BESACIER Laurent, et al.</auteurs>
<titre_de_la_source>Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP</titre_de_la_source>
<date_de_publication>2012</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00954193</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200711886U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00954193</reference_hal>
<references_archives_oai>oai:HAL:hal-00954193v1</references_archives_oai>
</informations_complementaires>
<resume>no abstract</resume>
<thematiques>Computer Science [cs]/Artificial Intelligence [cs.AI]</thematiques>
</article>
<article id="art79" titre="Formal Description of Resources for Ontology-based Semantic Annotation">
<auteurs>MA Yue, NAZARENKO Adeline, AUDIBERT Laurent</auteurs>
<titre_de_la_source>The seventh international conference on Language Resources and Evaluation</titre_de_la_source>
<date_de_publication>2010</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00528853</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112433P</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00528853</reference_hal>
<references_archives_oai>oai:HAL:hal-00528853v1</references_archives_oai>
</informations_complementaires>
<resume>Ontology-based semantic annotation aims at putting fragments of a text in correspondence with proper elements of an ontology such that the formal semantics encoded by the ontology can be exploited to represent text interpretation. In this paper, we formalize a resource for this goal. The main difficulty in achieving good semantic annotations consists in identifying fragments to be annotated and labels to be associated with them. To this end, our approach takes advantage of standard web ontology languages as well as rich linguistic annotation platforms. This in turn is concerned with how to formalize the combination of the ontological and linguistical information, which is a topical issue that has got an increasing discussion recently. Different from existing formalizations, our purpose is to extend ontologies by semantic annotation rules whose complexity increases along two dimensions: the linguistic complexity and the rule syntactic complexity. This solution allows reusing best NLP tools for the production of various levels of linguistic annotations. It also has the merit to distinguish clearly the process of linguistic analysis and the ontological interpretation.</resume>
<thematiques>Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art80" titre="TAL et linguistique de corpus pour aider la rédaction scientifique en anglais">
<auteurs>JACQUES Marie-Paule, HARTWELL Laura, FALAISE Achille</auteurs>
<titre_de_la_source>Actes de TALN 2013</titre_de_la_source>
<date_de_publication>2013</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00953809</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200711886U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00953809</reference_hal>
<references_archives_oai>oai:HAL:hal-00953809v1</references_archives_oai>
</informations_complementaires>
<resume>no abstract</resume>
<thematiques>Computer Science [cs]/Artificial Intelligence [cs.AI]</thematiques>
</article>
<article id="art81" titre="JEP-TALN-RECITAL 2012, Atelier TALAf 2012: Traitement Automatique des Langues Africaines">
<auteurs>ENGUEHARD Chantal, MANGEOT Mathieu, SÉRASSET Gilles</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2012</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00959247</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source></type_de_la_source>
<numero_national_de_structure_de_recherche>200711886U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00959247</reference_hal>
<references_archives_oai>oai:HAL:hal-00959247v1</references_archives_oai>
</informations_complementaires>
<resume>no abstract</resume>
<thematiques>Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art82" titre="Coherence and Cohesion for the Assessment of Text Readability">
<auteurs>TODIRASCU Amalia, FRANÇOIS Thomas, GALA Nuria, et al.</auteurs>
<titre_de_la_source>Proceedings of 10th International Workshop on Natural Language Processing and Cognitive Science (NLPCS 2013)</titre_de_la_source>
<date_de_publication>2013</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00860796</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>197217542U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00860796</reference_hal>
<references_archives_oai>oai:HAL:hal-00860796v1</references_archives_oai>
</informations_complementaires>
<resume>Text readability depends on a variety of variables. While lexico-semantic and syntactic factors have been widely used in the literature, more high-level discursive and cognitive properties such as cohesion and coherence have received little attention. This paper assesses the efficiency of 41 measures of text cohesion and text coherence as predictors of text readability. We compare results manually obtained on two corpora including texts with different difficulty levels and show that some cohesive features are indeed useful predictors.</resume>
<thematiques>Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art83" titre="Un outil de représentation et de développement des Grammaires de Propriétés">
<auteurs>GUÉNOT Marie-Laure, VANRULLEN Tristan</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2003</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00134193</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>201220301U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00134193</reference_hal>
<references_archives_oai>oai:HAL:hal-00134193v1</references_archives_oai>
</informations_complementaires>
<resume>We present in this paper a graphical tool for grammar development, based upon the Property Grammars formalism. We explain the reasons why the association of a complete and ergonomic representation and a neutral and homogeneous model, provides the considerable advantage of integrating information coming from descriptive linguistics.</resume>
<thematiques>Syntax, Natural language processing (NLP), Descriptive linguistics, Formal linguistics, Property Grammars (PG), Syntaxe, Traitement automatique des langues naturelles (TALN), Grammaires de Propriétés (GP), Linguistique descriptive, linguistique formelle, Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art84" titre="A Rule-Based Morphosemantic Analyzer for French for a Fine-Grained Semantic Annotation of Texts">
<auteurs>NAMER Fiammetta</auteurs>
<titre_de_la_source>SFCM</titre_de_la_source>
<date_de_publication>2013</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://halshs.archives-ouvertes.fr/halshs-00938945</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112505T</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>halshs-00938945</reference_hal>
<references_archives_oai>oai:HAL:halshs-00938945v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>NLP, morphosemantic approach, rule-based, French, derivation, neoclassical compounding, lexical-semantic feature, neologism, automatic definition, synonymy, hyponymy, co-hyponymy</thematiques>
</article>
<article id="art85" titre="Integrating learner corpora and natural language processing: A crucial step towards reconciling technological sophistication and pedagogical effectiveness">
<auteurs>GRANGER Sylvianne, KRAIF Olivier, PONTON Claude, et al.</auteurs>
<titre_de_la_source>ReCALL</titre_de_la_source>
<date_de_publication>2007</date_de_publication>
<numero>3</numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00359051</lien>
<issn>0958-3440</issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>199113164C</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00359051</reference_hal>
<references_archives_oai>oai:HAL:hal-00359051v1</references_archives_oai>
</informations_complementaires>
<resume>While the field of Technology-Enhanced Language Learning (TELL) is undeniably thriving, most technology-enhanced language tools are still relatively crude. One reason for this is that the field is disconnected from research in Natural Language Processing (NLP) and Corpus Linguistics (CL), two fields which could greatly improve the effectiveness of most pedagogical tools. The research carried out within the framework of the Kaleidoscope Network of Excellence aimed to demonstrate that it is both possible and desirable to integrate insights from NLP and CL into TELL to produce more powerful and effective tools. In the article we give a general outline of NLP and CL techniques and highlight their relevance for TELL. We also describe two types of integration that were implemented within the framework of Kaleidoscope: (1) integration of NLP processing into the glossary of the Moodle Learning Management System,  (2) integration of error-tagged learner corpus data into Exxelant, a web-based error interface for teachers and researchers. The chapter also argues the case for optimising the role of language in all technology-enhanced learning applications, whether language-focused or not.</resume>
<thematiques>Computer Science [cs]/Computation and Language [cs.CL], Computer Science [cs]/Technology for Human Learning</thematiques>
</article>
<article id="art86" titre="Improving Term Extraction with Terminological Resources">
<auteurs>AUBIN Sophie, HAMON Thierry</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2006</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00091444</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112433P</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00091444</reference_hal>
<references_archives_oai>oai:HAL:hal-00091444v1</references_archives_oai>
</informations_complementaires>
<resume>Studies of different term extractors on a corpus of the biomedical domain revealed decreasing performances when applied to highly technical texts. The difficulty or impossibility of customising them to new domains is an additional limitation. In this paper, we propose to use external terminologies to influence generic linguistic data in order to augment the quality of the extraction. The tool we implemented exploits testified terms at different steps of the process: chunking, parsing and extraction of term candidates. Experiments reported here show that, using this method, more term candidates can be acquired with a higher level of reliability. We further describe the extraction process involving endogenous disambiguation implemented in the term extractor YaTeA.</resume>
<thematiques>TAL, NLP, Acquisition terminologique, terminological knowledge acquisition, exploitation de ressources, extracteur de termes, term extractor, resources exploitation, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art87" titre="YaTeA - version 2006">
<auteurs>AUBIN Sophie, HAMON Thierry</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2006</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00090750</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source></type_de_la_source>
<numero_national_de_structure_de_recherche>200112433P</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00090750</reference_hal>
<references_archives_oai>oai:HAL:hal-00090750v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>NLP, Terminology, Term extraction, Computer Science [cs]/Artificial Intelligence [cs.AI]</thematiques>
</article>
<article id="art88" titre="Expressivity and comparison of models of discourse structure">
<auteurs>ANTOINE Venant, ASHER Nicholas, MULLER Philippe, et al.</auteurs>
<titre_de_la_source>Special Interest Group on Discourse and Dialogue</titre_de_la_source>
<date_de_publication>2013</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.inria.fr/hal-00838260</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>199511949P</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00838260</reference_hal>
<references_archives_oai>oai:HAL:hal-00838260v1</references_archives_oai>
</informations_complementaires>
<resume>Several discourse annotated corpora now exist for NLP exploitation. Nevertheless, it is not clear how these annotations compare: are they incompatible, incomparable, or do they share some inter- pretations? In this paper, we relate three types of discourse annotation as found in: (i) the RST Tree Bank corpus, (ii) SDRT corpora DISCOR and ANNODIS, and (iii) dependency tree structures. The latter have not yet been used in actual annotations, but represent elementary substructures which are interesting for automated parsing. Specifically, we discuss two ways of interpreting RST trees by taking discourse relations as semantics operators, one is fully specified, the other one underspecified. We also provide an underspecified semantic interpretation of dependency trees. We define trans- lations between RST and DT that preserve these underspecified interpretations. On this basis, we design similarity measures that quantify the loss of information implied by these translations. Over- all, these translations and metrics provide a unified framework that will hopefully enable us to take advantage of the various existing discourse annotation data that are available for automated tasks.</resume>
<thematiques>Computer Science [cs]/Document and Text Processing, Computer Science [cs]/Artificial Intelligence [cs.AI]</thematiques>
</article>
<article id="art89" titre="Le TALN au service de la didactique du français langue étrangère écrit">
<auteurs>AUDRAS Isabelle, GANASCIA Jean-Gabriel</auteurs>
<titre_de_la_source>The 13th Conference on Natural Language Processing (TALN 2006). April 10-13, 2006. Leuven (Belgium)</titre_de_la_source>
<date_de_publication>2006</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://telearn.archives-ouvertes.fr/hal-00197395</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>199712651U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00197395</reference_hal>
<references_archives_oai>oai:HAL:edutice-00086950v1, oai:HAL:hal-00197395v1</references_archives_oai>
</informations_complementaires>
<resume>New text analysis softwares issued from fields of research such as Machine Learning and Natural Languages Processing prove to be relevant tools for the language sciences. Littératron is a new data-processing tool for the automatic extraction of syntactic patterns, designed at LIP6 by Jean-Gabriel Ganascia. Associated with a linear text analyser, it reveals the stylistic peculiarities of a text. We will see that Littératron carries out a linguistic diagnosis of learners if used in language sciences, especially in the field of acquisition of written French as a foreign language. The learner can be from a heterogeneous group (various language levels and various mother tongues) or from a homogeneous group (only one language level and one mother tongue, here, Arabic). The interest of this approach is related to three fields: first, language didactics, on a purely educational basis,  next, computational linguistics,  finally, computer-assisted learning.</resume>
<thematiques>natural language processing (NLP), linguistic diagnosis, foreign language learning, stylistics, to write in a foreign language, extraction of recurrent patterns, Computer Science [cs]/Technology for Human Learning</thematiques>
</article>
<article id="art90" titre="Dialogar é preciso" sous_titre="Linguística para processamento de línguas">
<auteurs>LAPORTE Eric, SMARSARO Aucione, VALE Oto</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2013</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal-upec-upem.archives-ouvertes.fr/hal-00804609</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source></type_de_la_source>
<numero_national_de_structure_de_recherche>200212717U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00804609</reference_hal>
<references_archives_oai>oai:HAL:hal-00804609v1</references_archives_oai>
</informations_complementaires>
<resume>The 1st Conference on Linguistics for Automatic Natural Language Processing (LiPrAL) was held in the Universidade Federal do Espírito santo (UFES). Knowledge about specifically linguistic concepts is essential to the creation of natural language processing (NLP) systems. Presently, applications are designed nearly only by computer scientists which, in general, feel alien to linguistics and resort to scarce linguistic concepts. The performances of resulting computational systems could be substantially improved if more research invested into exhaustive coverage of the lexicon of languages. We think that linguists able to describe linguistic forms and to formalize their results for computational use can be hired into technological companies in the NLP field. Therefore, studies in this area, in addition to contributing to language teaching, have a potential of jobs in technological activities, which is not realised by the academic community yet. In order to reverse this situation, it is necessary to explain and prove the crucial relevance of research in linguistics for NLP, to invest more in researcher motivation and to investigate more into description of formalized structures.</resume>
<thematiques>Natural language processing dictionary, Language resource, Computational linguistics, Grammar, Corpus, Humanities and Social Sciences/Linguistics, Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art91" titre="Natural language processing of radiology reports for the detection of thromboembolic diseases and clinically relevant incidental findings">
<auteurs>PHAM Anne-Dominique, NÉVÉOL Aurélie, LAVERGNE Thomas, et al.</auteurs>
<titre_de_la_source>BMC Bioinformatics</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination>266</pagination>
<lien>http://dx.doi.org/10.1186/1471-2105-15-266</lien>
<issn>1471-2105</issn>
<informations_complementaires>
<type_de_publication>&article;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>201019165T, 197217542U</numero_national_de_structure_de_recherche>
<doi>10.1186/1471-2105-15-266</doi>
<reference_hal>inserm-01094167</reference_hal>
<references_archives_oai>oai:HAL:inserm-01094167v1</references_archives_oai>
</informations_complementaires>
<resume>Background:Natural Language Processing (NLP) has been shown effective to analyze the content of radiologyreports and identify diagnosis or patient characteristics. We evaluate the combination of NLP and machine learningto detect thromboembolic disease diagnosis and incidental clinically relevant findings from angiography andvenography reports written in French. We model thromboembolic diagnosis and incidental findings as a set of concepts,modalities and relations between concepts that can be used as features by a supervised machine learning algorithm. Acorpus of 573 radiology reports was de-identified and manually annotated with the support of NLP tools by a physicianfor relevant concepts, modalities and relations. A machine learning classifier was trained on the dataset interpreted by aphysician for diagnosis of deep-vein thrombosis, pulmonary embolism and clinically relevant incidental findings. Decisionmodels accounted for the imbalanced nature of the data and exploited the structure of the reports.Results:The best model achieved an F measure of 0.98 for pulmonary embolism identification, 1.00 for deep veinthrombosis, and 0.80 for incidental clinically relevant findings. The use of concepts, modalities and relations improvedperformances in all cases.Conclusions:This study demonstrates the benefits of developing an automated method to identify medical concepts,modality and relations from radiology reports in French. An end-to-end automatic system for annotationand classification which could be applied to other radiology reports databases would be valuable for epidemiologicalsurveillance, performance monitoring, and accreditation in French hospitals.</resume>
<thematiques>Life Sciences [q-bio]/Public Health and Epidemiology</thematiques>
</article>
<article id="art92" titre="Fusion of hyperspectracl and panchromatic images: a hybrid use of indusion and nonlinear PCA">
<auteurs>LICCIARDI G., KHAN M.M., CHANUSSOT Jocelyn</auteurs>
<titre_de_la_source>ICIP 2012 - 19th IEEE International Conference on Image Processing</titre_de_la_source>
<date_de_publication>2012</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00799633</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200711885T</numero_national_de_structure_de_recherche>
<doi>10.1109/ICIP.2012.6467314</doi>
<reference_hal>hal-00799633</reference_hal>
<references_archives_oai>oai:HAL:hal-00799633v1</references_archives_oai>
</informations_complementaires>
<resume>Generally, for optical satellite sensors spatial and spectral resolutions are highly correlated factors. In fact, given the design constraints of these sensors, there is an inverse relation between their spatial and spectral resolution. Thus, the hyperspectral sensors have a high spectral resolution i.e. large number of bands covering the electromagnetic spectrum, but a lower spatial resolution. On the other hand, panchromatic (PAN) images have the highest spatial resolution but no spectral diversity. For better utilization and interpretation, hyperspectral images having both high spectral and spatial resolution are desired. This can be achieved by making use of a high spatial resolution PAN image in the context of pansharpening or image fusion. Several fusion approaches have been proposed in the literature. In this paper we propose the use of a hybrid algorithm combining substitution and injection methods. One of the main challenges in hyperspectral image fusion is the improvement of the spatial resolution, i.e. spatial details while preserving the original spectral information. This requires addition of pertinent spatial details to each band of the HS image. However, due to large number of bands the pansharpening of HS images is computationally expensive. Thus a dimensionality reduction preprocess, compressing the original number of measurements into a lower dimensional space, becomes mandatory. In this paper we propose the use of non-linear principal components instead of the original HS bands as input to a fusion process to enhance the spatial resolution of the HS image.</resume>
<thematiques>NLPCA, Hyperspectral image, Pansharpening, Image fusion, Engineering Sciences [physics]/Signal and Image processing, Computer Science [cs]/Signal and Image Processing</thematiques>
</article>
<article id="art93" titre="Modeling the Complexity of Manual Annotation Tasks: a Grid of Analysis">
<auteurs>FORT Karën, NAZARENKO Adeline, ROSSET Sophie</auteurs>
<titre_de_la_source>International Conference on Computational Linguistics</titre_de_la_source>
<date_de_publication>2012</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00769631</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112433P, 198912571S, 197217542U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00769631</reference_hal>
<references_archives_oai>oai:HAL:hal-00769631v1</references_archives_oai>
</informations_complementaires>
<resume>Manual corpus annotation is getting widely used in Natural Language Processing (NLP). While being recognized as a difficult task, no in-depth analysis of its complexity has been performed yet. We provide in this article a grid of analysis of the different complexity dimensions of an annotation task, which helps estimating beforehand the difficulties and cost of annotation campaigns. We observe the applicability of this grid on existing annotation campaigns and detail its application on a real-world example.</resume>
<thematiques>manual corpus annotation, annotation campaign management, annotation cam- paign cost estimate, Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art94" titre="An Algerian dialect: Study and Resources">
<auteurs>HARRAT Salima, MEFTOUH Karima, ABBAS Mourad, et al.</auteurs>
<titre_de_la_source>International Journal of Advanced Computer Science and Applications - IJACSA</titre_de_la_source>
<date_de_publication>2016</date_de_publication>
<numero>3</numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01297415</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>198912571S</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01297415</reference_hal>
<references_archives_oai>oai:HAL:hal-01297415v1</references_archives_oai>
</informations_complementaires>
<resume>Arabic is the official language overall Arab countries,it is used for official speech, news-papers, public administrationand school. In Parallel, for everyday communication, nonofficialtalks, songs and movies, Arab people use their dialectswhich are inspired from Standard Arabic and differ from oneArabic country to another. These linguistic phenomenon is calleddisglossia, a situation in which two distinct varieties of a languageare spoken within the same speech community. It is observedThroughout all Arab countries, standard Arabic widely writtenbut not used in everyday conversation, dialect widely spoken ineveryday life but almost never written. Thus, in NLP area, a lotof works have been dedicated for written Arabic. In contrast,Arabic dialects at a near time were not studied enough. Interestfor them is recent. First work for these dialects began in the lastdecade for middle-east ones. Dialects of the Maghreb are justbeginning to be studied. Compared to written Arabic, dialectsare under-resourced languages which suffer from lack of NLPresources despite their large use. We deal in this paper withArabic Algerian dialect a non-resourced language for which noknown resource is available to date. We present a first linguisticstudy introducing its most important features and we describethe resources that we created from scratch for this dialect.</resume>
<thematiques>Morphological Analysis, Arabic dialect, Algerian dialect, Modern Standard Arabic, Grapheme to Phoneme Conversion, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art95" titre="The plant RWP-RK transcription factors: key regulators of nitrogen responses and of gametophyte development">
<auteurs>CHARDIN Camille, GIRIN Thomas, ROUDIER Francois, et al.</auteurs>
<titre_de_la_source>Journal of Experimental Botany</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero>19</numero>
<pagination>5577-5587</pagination>
<lien>http://prodinra.inra.fr/record/289854</lien>
<issn>0022-0957</issn>
<informations_complementaires>
<type_de_publication>&article;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>201119640E</numero_national_de_structure_de_recherche>
<doi>10.1093/jxb/eru261</doi>
<reference_hal>hal-01204150</reference_hal>
<references_archives_oai>oai:prodinra.inra.fr:289854, oai:HAL:hal-01204150v1</references_archives_oai>
</informations_complementaires>
<resume>The plant specific RWP-RK family of transcription factors, initially identified in legumes and Chlamydomonas, are found in all vascular plants, green algae, and slime molds. These proteins possess a characteristic RWP-RK motif, which mediates DNA binding. Based on phylogenetic and domain analyses, we classified the RWP-RK proteins of six different species in two subfamilies: the NIN-like proteins (NLPs), which carry an additional PB1 domain at their C-terminus, and the RWP-RK domain proteins (RKDs), which are divided into three subgroups. Although, the functional analysis of this family is still in its infancy, several RWP-RK proteins have a key role in regulating responses to nitrogen availability. The nodulation-specific NIN proteins are involved in nodule organogenesis and rhizobial infection under nitrogen starvation conditions. Arabidopsis NLP7 in particular is a major player in the primary nitrate response. Several RKDs act as transcription factors involved in egg cell specification and differentiation or gametogenesis in algae, the latter modulated by nitrogen availability. Further studies are required to extend the general picture of the functional role of these exciting transcription factors.</resume>
<thematiques>transcription factors, RWP-RK family, nitrogen, NLP, RKD proteins, regulation, facteur de transcription, azote, protéine, gamétophyte, chlamydomonas, légumineuse, plante vasculaire, algue verte, myxomycète, analyse phylogénétique</thematiques>
</article>
<article id="art96" titre="Automatic Method Of Domain Ontology Construction based on Characteristics of Corpora POS-Analysis">
<auteurs>OROBINSKA Olena</auteurs>
<titre_de_la_source>XV Russian conference Internet and Modern Society</titre_de_la_source>
<date_de_publication>2012</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00987456</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>199914397H</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00987456</reference_hal>
<references_archives_oai>oai:HAL:hal-00987456v1</references_archives_oai>
</informations_complementaires>
<resume>It is now widely recognized that ontologies, are one of the fundamental cornerstones of knowledge-based systems. What is lacking, however, is a currently accepted strategy of how to build ontology,  what kinds of the resources and techniques are indispensables to optimize the expenses and the time on the one hand and the amplitude, the completeness, the robustness of en ontology on the other hand. The paper offers a semi-automatic ontology construction method from text corpora in the domain of radiological protection. This method is composed from next steps: 1) text annotation with part-of-speech tags,  2) revelation of the significant linguistic structures and forming the templates,  3) search of text fragments corresponding to these templates,  4) basic ontology instantiation process</resume>
<thematiques>Information Extraction, NLP, Relation Recognition Rules, Instance Recognition Rules, Semantic Analysis, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art97" titre="Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL">
<auteurs>MOLINAMEJIA JorgeMauricio, SCHWAB Didier, SÉRASSET Gilles</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2012</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00959243</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source></type_de_la_source>
<numero_national_de_structure_de_recherche>200711886U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00959243</reference_hal>
<references_archives_oai>oai:HAL:hal-00959243v1</references_archives_oai>
</informations_complementaires>
<resume>no abstract</resume>
<thematiques>Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art98" titre="Composing Relationships with Translations">
<auteurs>GARCÍA-DURÁN Alberto, BORDES Antoine, USUNIER Nicolas</auteurs>
<titre_de_la_source>Conference on Empirical Methods in Natural Language Processing (EMNLP 2015)</titre_de_la_source>
<date_de_publication>2015</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01167811</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>201220263C</numero_national_de_structure_de_recherche>
<doi>10.18653/v1/D15-1034</doi>
<reference_hal>hal-01167811</reference_hal>
<references_archives_oai>oai:HAL:hal-01301243v1, oai:HAL:hal-01167811v1</references_archives_oai>
</informations_complementaires>
<resume>Performing link prediction in Knowledge Bases (KBs) with embedding-based models , like with the model TransE (Bordes et al., 2013) which represents relationships as translations in the embedding space, have shown promising results in recent years. Most of these works focused on modeling single relationships and hence do not take full advantage of the graph structure of KBs. In this paper, we propose an extension of TransE that learns to explicitly model composition of relationships via the addition of their corresponding translation vectors. We show empirically that this allows to improve performance for predicting single relationships as well as compositions of pairs of them.</resume>
<thematiques>Computer Science [cs]</thematiques>
</article>
<article id="art99" titre="Medical Imaging Report Indexing: Enrichment of Index through an Algorithm of Spreading over a Lexico-semantic Network">
<auteurs>LAFOURCADE Mathieu, RAMADIER Lionel</auteurs>
<titre_de_la_source>RANLP: Recent Advances in Natural Language Processing</titre_de_la_source>
<date_de_publication>2015</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01300405</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>199111950H</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01300405</reference_hal>
<references_archives_oai>oai:HAL:hal-01300405v1</references_archives_oai>
</informations_complementaires>
<resume>In medical imaging domain, digitized data is rapidly expanding Therefore it is of major interest for radiologists to be able to do an efficient and accurate extraction of imaging and clinical data (radiology reports) which are essential for a rigorous diagnosis and for a better management of patients. In daily practice, radiology reports are written using a non-standardized language which is often ambiguous and noisy. The queries of radiological images can be greatly facilitated through textual indexing of associated reports. In order to improve the quality of the analysis of such reports, it is desirable to specify an index enlargement algorithm based on spreading activations over a general lexical-semantic network. In this paper, we present such an algorithm along with its qualitative evaluation .</resume>
<thematiques>Computer Science [cs]/Artificial Intelligence [cs.AI], Computer Science [cs]/Medical Imaging, Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art100" titre="Endogeneous Consolidation of Lexical Semantic Networks" sous_titre="Inference and annotation of semantic relations, inference rules and Domain-Specific Language">
<auteurs>ZARROUK Manel</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2015</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>http://hal-lirmm.ccsd.cnrs.fr/tel-01300285</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source></type_de_la_source>
<numero_national_de_structure_de_recherche>199111950H</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>tel-01300285</reference_hal>
<references_archives_oai>oai:HAL:tel-01300285v1</references_archives_oai>
</informations_complementaires>
<resume>Developing lexico-semantic resources is a major issue in the Natural Language Processing field. These resources, by making explicit, interalia, some knowledge possessed only by humans, aim at providing the ability of a precise and complete text understanding to NLP tasks. Popular resources-building strategies involving crowdsourcing are flowering in NLP and are proved to be successful. However, the resulted resources are not free of errors and lack some important semantic relations. In this PhD thesis, we used the French lexical semantic network from the project JeuxDeMots as a case-study. We designed an endogenous consolidation system for this type of networks based on inferring and annotating new semantic relations using the already existing ones, as well as extracting and proposing inference rules able to (re)generate a considerable part of the network. In addition, we conceived a domain specific language for manipulating the consolidation system along with the network itself and a prototype was implemented</resume>
<thematiques>NLP, lexical-semantic networks, Consolidation, relations inference, relations annotation, Inference rules, Domain specific language, langage dédié, règles d’inférence, annotation de relations, inférence de relations, TALN, réseaux  lexico-sémantiques, Computer Science [cs]/Computation and Language [cs.CL], Computer Science [cs]</thematiques>
</article>
<article id="art101" titre="Webaffix: Discovering Morphological Links on the WWW">
<auteurs>HATHOUT Nabil, TANGUY Ludovic</auteurs>
<titre_de_la_source>LREC 2002</titre_de_la_source>
<date_de_publication>2002</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://halshs.archives-ouvertes.fr/halshs-01322326</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200711926M</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>halshs-01322326</reference_hal>
<references_archives_oai>oai:HAL:halshs-01322326v1</references_archives_oai>
</informations_complementaires>
<resume>This paper presents a new language-independent method for finding morphological links between newly appeared words (i.e. absent from reference word lists). Using the WWW as a corpus, the Webaffix tool detects the occurrences of new derived lexemes based on a given suffix, proposes a base lexeme following a standard scheme (such as noun-verb), and then performs a compatibility test on the word pairs produced, using the Web again, but as a source of cooccurrences. The resulting pairs of words are used to build generic morphological databases useful for a number of NLP tasks. We develop and comment an example use of Webaffix to find new noun/verb pairs in French.</resume>
<thematiques>Morphology, Web as corpus, Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art102" titre="Natural Language Processing for aviation safety reports: from classification to interactive analysis">
<auteurs>TANGUY Ludovic, TULECHKI Nikola, URIELI Assaf, et al.</auteurs>
<titre_de_la_source>Computers in Industry</titre_de_la_source>
<date_de_publication>2016</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://halshs.archives-ouvertes.fr/halshs-01322238</lien>
<issn>0166-3615</issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>200711926M</numero_national_de_structure_de_recherche>
<doi>10.1016/j.compind.2015.09.005</doi>
<reference_hal>halshs-01322238</reference_hal>
<references_archives_oai>oai:HAL:halshs-01322238v1</references_archives_oai>
</informations_complementaires>
<resume>In this paper we describe the different NLP techniques designed and used in collaboration between the CLLE-ERSS research laboratory and the CFH / Safety Data company to manage and analyse aviation incident reports. These reports are written every time anything abnormal occurs during a civil air flight. Although most of them relate routine problems, they are a valuable source of information about possible sources of greater danger. These texts are written in plain language, show a wide range of linguistic variation (telegraphic style overcrowded by acronyms or standard prose) and exist in different languages, even for a single company/country (although our main focus is on English and French). In addition to their variety, their sheer quantity (e.g. 600/month for a large airline company) clearly requires the use of advanced NLP and text mining techniques in order to extract useful information from them. Although this context and objectives seem to indicate that standard NLP techniques can be applied in a straightforward manner, innovative techniques are required to handle the specifics of aviation report text and the complex classification systems. We present several tools that aim at a better access to this data (classification and information retrieval), and help aviation safety experts in their analyses (data/text mining and interactive analysis). Some of these tools are currently in test or in use both at the national and international levels, by airline companies as well as by regulation authorities (DGAC, EASA , ICAO).</resume>
<thematiques>Safety reports, aviation, NLP, document classification, text mining, Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art103" titre="La base lexicale Démonette : entre sémantique constructionnelle et morphologie dérivationnelle">
<auteurs>HATHOUT Nabil, NAMER Fiammetta</auteurs>
<titre_de_la_source>TALN</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://halshs.archives-ouvertes.fr/halshs-01110398</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200711926M, 200112505T</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>halshs-01110398</reference_hal>
<references_archives_oai>oai:HAL:halshs-01110398v1</references_archives_oai>
</informations_complementaires>
<resume>De´monette is a lexical database whose vertices (lexical entries) and edges (morphological relations between the vertices) are annotated with morpho-semantic information. It results from an original design incorporating two radically different approaches: Morphonette, a resource based on derivational analogies and De´riF, an analyzer based on linguistic rules. However, Daemonette is not a simple merger of two pre-existing ressources: its architecture is fully compatible with the lexematic approach to morphology,  its contents can be extended using data from various other sources. The article presents the De´monette model and the content of its current version, including 31,204 verbs, action nouns, agent nouns and property adjectives, where morphological links between both direct ascendants and indirectly related words have bi-oriented definitions. Finally, De´monette is assessed with respect to Verbaction with a recall of 84% and a precision of 90%.</resume>
<thematiques>Lexical Network. Derivational morphology. Morphological family. Lexical semantics. French., Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art104" titre="Mbochi : corpus oral, traitement automatique et exploration phonologique">
<auteurs>RIALLAND Annie, EMBANGAABOROBONGUI Martial, LAMEL Lori</auteurs>
<titre_de_la_source>JEP-TALN-RECITAL</titre_de_la_source>
<date_de_publication>2012</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01137994</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112425F, 197217542U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01137994</reference_hal>
<references_archives_oai>oai:HAL:hal-01137994v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>corpus oral, phonologie, Mbochi, Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art105" titre="Continue or Stop Reading? Modeling Decisions in Information Search">
<auteurs>LOPEZOROZCO Francisco, GUÉRIN-DUGUÉ Anne, LEMAIRE Benoît</auteurs>
<titre_de_la_source>9th International Workshop on Natural Language Processing and Cognitive Science (NLPCS 2012)</titre_de_la_source>
<date_de_publication>2012</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00790210</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200711885T, 199911790Z</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00790210</reference_hal>
<references_archives_oai>oai:HAL:hal-00790210v1</references_archives_oai>
</informations_complementaires>
<resume>This paper presents a cognitive computational model of the way people read a paragraph with the task of quickly deciding whether it is better related to a given goal than another paragraph processed previously. In particular, the model attempts to predict the time at which participants would decide to stop reading the current paragraph because they have enough information to make their decision. We proposed a two-variable linear threshold to account for that decision, based on the rank of the fixation and the difference of semantic similarities between each paragraph and the goal. Our model performance is compared to the eye tracking data of 22 participants.</resume>
<thematiques>decision making, information search, reading, text, computational model, Cognitive science/Computer science, Cognitive science/Psychology</thematiques>
</article>
<article id="art106" titre="How can catchy titles be generated without loss of informativeness?">
<auteurs>LOPEZ Cédric, PRINCE Violaine, ROCHE Mathieu</auteurs>
<titre_de_la_source>Expert Systems with Applications</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero>4</numero>
<pagination></pagination>
<lien>http://hal-lirmm.ccsd.cnrs.fr/lirmm-00856372</lien>
<issn>0957-4174</issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>200718239Z, 199111950H</numero_national_de_structure_de_recherche>
<doi>10.1016/j.eswa.2013.07.102</doi>
<reference_hal>lirmm-00856372</reference_hal>
<references_archives_oai>oai:HAL:lirmm-00856372v1</references_archives_oai>
</informations_complementaires>
<resume>Automatic titling of text documents is an essential task for several applications (automatic heading of e-mails, summarization, and so forth). This paper describes a system facilitating information retrieval in a set of textual documents by tackling the automatic titling and subtitling issue. Automatic titling here involves providing both informative and catchy titles. We thus propose two different approaches based on NLP, text mining, and Web Mining techniques. The first one (POSTIT) consists of extracting relevant noun phrases from texts as candidate titles. An original approach combining statistical criteria and noun phrase positions in the text helps in collecting informative titles and subtitles. The second approach (NOMIT) is based on various assumptions made on POSTIT and aims to generate both informative and catchy titles. Both approaches are applied to a corpus of news articles, then evaluated according to two criteria, i.e. informativeness and catchiness.</resume>
<thematiques>Computer Science [cs]/Web, Computer Science [cs]/Information Retrieval [cs.IR], Computer Science [cs]/Artificial Intelligence [cs.AI]</thematiques>
</article>
<article id="art107" titre="Predicting the relevance of distributional semantic similarity with contextual information">
<auteurs>MULLER Philippe, FABRE Cécile, ADAM Clémentine</auteurs>
<titre_de_la_source>52nd Annual Meeting of the Association for Computational Linguistics - ACL 2014</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01056292</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200711926M, 199511949P</numero_national_de_structure_de_recherche>
<doi>10.3115/v1/P14-1045</doi>
<reference_hal>hal-01056292</reference_hal>
<references_archives_oai>oai:HAL:hal-01056292v1, oai:HAL:hal-01259674v1</references_archives_oai>
</informations_complementaires>
<resume>Using distributional analysis methods to compute semantic proximity links between words has become commonplace in NLP. The resulting relations are often noisy or difficult to interpret in general. This paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains, but instead of considering it in abstracto, we focus on pairs of words in context. In a discourse, we are interested in knowing if the semantic link between two items is a byproduct of textual coherence or is irrelevant. We first set up a human annotation of semantic links with or without contextual information to show the importance of the textual context in evaluating the relevance of semantic similarity, and to assess the prevalence of actual semantic relations between word tokens. We then built an experiment to automatically predict this relevance, evaluated on the reliable reference data set which was the outcome of the first annotation. We show that in-document information greatly improve the prediction made by the similarity level alone.</resume>
<thematiques>distributional semantics, evaluation, Computer Science [cs]/Document and Text Processing, Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art108" titre="Representation of Linguistic and Domain Knowledge for Second Language Learning in Virtual Worlds">
<auteurs>DENIS Alexandre, FALK Ingrid, GARDENT Claire, et al.</auteurs>
<titre_de_la_source>LREC - The eighth international conference on Language Resources and Evaluation - 2012</titre_de_la_source>
<date_de_publication>2012</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.inria.fr/hal-00766418</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>198912571S</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00766418</reference_hal>
<references_archives_oai>oai:HAL:hal-00766418v1</references_archives_oai>
</informations_complementaires>
<resume>There has been much debate, both theoretical and practical, on how to link ontologies and lexicons in natural language processing (NLP) applications. In this paper, we focus on an application in which lexicon and ontology are used to generate teaching material. We briefly describe the application (a serious game for language learning). We then zoom in on the representation and interlinking of the lexicon and of the ontology. We show how the use of existing standards and of good practice principles facilitates the design of our resources while satisfying the expressivity requirements set by natural language generation.</resume>
<thematiques>virtual environments, ontology-lexicon interface, natural language generation, Computer Science [cs]/Computation and Language [cs.CL], Cognitive science/Computer science</thematiques>
</article>
<article id="art109" titre="Specific ontologies for semantic indexing from natural language properties">
<auteurs>SIDHOM Sahbi, KHEMIRI Nabil</auteurs>
<titre_de_la_source>EJDE - Electronic Journal of Digital Enterprise (ISSN: 1776-2960)</titre_de_la_source>
<date_de_publication>2012</date_de_publication>
<numero>1776-2960 R291 (ISSN)</numero>
<pagination></pagination>
<lien>https://hal.inria.fr/hal-00782918</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>198912571S</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00782918</reference_hal>
<references_archives_oai>oai:HAL:hal-00782918v1</references_archives_oai>
</informations_complementaires>
<resume>In this paper, we present a specific ontologies based on the extraction of semantic information in text documents. Document here concerns the context of the valorization of Tunisian patrimony. As approach, we propose to represent semantic properties in document contents from heterogeneous information (multimedia) concerning by the patrimony objects. For indexing and information retrieval (IR), we develop processes based on the noun phrase (NP) properties and their semantic representations. These processes use natural language processing (NLP) to take into account the NP syntactic and semantic structures. In view of this study, the specific ontology designed has the encapsulation principle to capitalize the concept and knowledge as NP and its semantic relations.</resume>
<thematiques>NooJ platform, ontology, indexing process, information retrieval (IR), noun phrase (NP), natural language processing (NLP), semantic relations, NP parser, NooJ platform., Computer Science [cs]/Artificial Intelligence [cs.AI]</thematiques>
</article>
<article id="art110" titre="Introducing PersPred, a syntactic and semantic database for Persian Complex Predicates">
<auteurs>SAMVELIAN Pollet, FAGHIRI Pegah</auteurs>
<titre_de_la_source>The 9th Workshop on Multiword Expressions</titre_de_la_source>
<date_de_publication>2013</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://halshs.archives-ouvertes.fr/halshs-00974259</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112497J, 199712589B</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>halshs-00974259</reference_hal>
<references_archives_oai>oai:HAL:halshs-00974259v1</references_archives_oai>
</informations_complementaires>
<resume>This paper introduces PersPred, the first manually elaborated syntactic and semantic database for Persian Complex Predicates (CPs). Beside their theoretical interest, Persian CPs constitute an important challenge in Persian lexicography and for NLP. The first delivery, PersPred 1, contains 700 CPs, for which 22 fields of lexical, syntactic and semantic information are encoded. The semantic classification PersPred provides allows to account for the productivity of these combinations in a way which does justice to their compositionality without overlooking their idiomaticity.</resume>
<thematiques>Persian, Complex Predicates, Mutliword Expressions, Lexicon, Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art111" titre="Evaluation  of automatic speech recognition systems according to the applicatives cases">
<auteurs>BENJANNET MohamedAmer</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2015</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://tel.archives-ouvertes.fr/tel-01240064</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source></type_de_la_source>
<numero_national_de_structure_de_recherche>197217542U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>tel-01240064</reference_hal>
<references_archives_oai>oai:HAL:tel-01240064v1</references_archives_oai>
</informations_complementaires>
<resume>It is important to regularly assess the technological innovation products in order to estimate the level of maturity reached by the technology and study the applications frameworks in which they can be used. Natural language processing (NLP) aims at developing modules and applications that automatically process the human language. That makes the field relevant to beth research and technological innovation. For years, the different technological modules from the NLP were developed separately. Therefore, the existing evaluation methods are in most modular. They allow to evaluate only one module at a time, while today, many applications need to combine several NLP modules to solve complex tasks. The new challenge in terms of evaluation is then to evaluate the different modules while taking into account the applicative context.Our work addresses the evaluation of Automatic Speech Recognition (ASR) systems according to the applicative context. We will focus on the case of Named Entities Recognition (NER) from spoken documents transcriped automatically. In the first part, we address the issue of evaluating ASR systems according to the application context through a study of the state of the art. We describes the tasks of ASR and NER proposed during several evalution campaigns and we discuss the protocols established for their evaluation. We also point the limitations of modular evaluation approaches and we expose the alternatives measures proposed in the literature. In the second part we describe the studied task of named entities detection, classification and decomposition and we propose a new metric ETER (Entity Tree Error Rate) which allows to take into account the specificity of the task and the applicative context during the evaluation. ETER also eliminates the biases observed with the existing metrics. In the third part, we define a new measure ATENE (Automatic Transcriptions Evaluation for Named Entities) that evaluates the quality of ASR systems and the impact of their errors for REN systems applied downstream. Rather than directly comparing reference and hypothesis transcriptions, ATENE measure how harder it becames to  identify entities given the differences between hypothesis and reference by comparing an estimated likelihood of presence of entities. It is composed of two elementary measurements. The first aims to assess the risk of entities deletions and substitutions and the second aims to assess the risk of entities insertions caused by ASR errors.Our validation experiments show that the measurements given by ATENE correlate better than other measures from the state of the art with the performance of REN systems.</resume>
<thematiques>Metric, Ner, Asr, Métrique, Évaluation, Atene, Rap, Ren, Computer Science [cs]/Computation and Language [cs.CL], Computer Science [cs]/Document and Text Processing, Computer Science [cs]/Information Retrieval [cs.IR], Computer Science [cs]/Performance [cs.PF]</thematiques>
</article>
<article id="art112" titre="Detection of Anomalies Produced by Buried Archaeological Structures Using Nonlinear Principal Component Analysis Applied to Airborne Hyperspectral Image">
<auteurs>CAVALLI RosaMaria, LICCIARDI Giorgio, CHANUSSOT Jocelyn</auteurs>
<titre_de_la_source>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</titre_de_la_source>
<date_de_publication>2013</date_de_publication>
<numero>2</numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00798517</lien>
<issn>1939-1404</issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>200711885T</numero_national_de_structure_de_recherche>
<doi>10.1109/JSTARS.2012.2227301</doi>
<reference_hal>hal-00798517</reference_hal>
<references_archives_oai>oai:HAL:hal-00798517v1</references_archives_oai>
</informations_complementaires>
<resume>In this paper, airborne hyperspectral data have been exploited by means of Nonlinear Principal Component Analysis (NLPCA) to test their effectiveness as a tool for archaeological prospection, evaluating their potential for detecting anomalies related to buried archaeological structures. In the literature, the NLPCA was used to decorrelate the information related to a hyperspectral image. The resulting nonlinear principal components (NLPCs) contain information related to different land cover types and biophysical properties, such as vegetation coverage or soil wetness. From this point of view, NLPCA applied to airborne hyperspectral data was exploited to test their effectiveness and capability in highlighting the anomalies related to buried archaeological structures. Each component obtained from the NLPCA has been interpreted in order to assess any tonal anomalies. As a matter of a fact, since every analyzed component exhibited anomalies different in terms of size and intensity, the Separability Index (SI) was applied for measuring the tonal difference of the anomalies with respect to the surrounding area. SI has been evaluated for determining the potential of anomalies detection in each component. The airborne Multispectral Infrared and Visible Imaging Spectrometer (MIVIS) images, collected over the archaeological Park of Selinunte, were analyzed for this purpose. In this area, the presence of remains, not yet excavated, was reported by archaeologists. A previous analysis of this image, carried out to highlight the buried structures, appear to match the archaeological prospection. The results obtained by the present work demonstrate that the use of the NLPCA technique, compared to previous approaches emphasizes the ability of airborne hyperspectral images to identify buried structures. In particular, the adopted data processing flow chart (i.e. NLPCA and SI techniques, data resampling criteria and anomaly evaluations criteria) applied to MIVIS airborne hyperspectral data, collected over Selinunte Archaeological Park, highlighted the ability of the NLPCA technique in emphasizing the anomalies related to the presence of buried structure.</resume>
<thematiques>Hyperspectral image processing, nonlinear PCA, anomaly detection, archaeological prospection, Engineering Sciences [physics]/Signal and Image processing, Computer Science [cs]/Signal and Image Processing</thematiques>
</article>
<article id="art113" titre="A Local Latent Semantic Analysis-based Kernel for Document Similarities">
<auteurs>ASEERVATHAM Sujeevan</auteurs>
<titre_de_la_source>IEEE International Joint Conference on Neural Networks</titre_de_la_source>
<date_de_publication>2008</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00339857</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112433P</numero_national_de_structure_de_recherche>
<doi>10.1109/IJCNN.2008.4633792</doi>
<reference_hal>hal-00339857</reference_hal>
<references_archives_oai>oai:HAL:hal-00339857v1</references_archives_oai>
</informations_complementaires>
<resume>The document similarity measure is a key point in textual data processing. It is the main responsible of the performance of a processing system. Since a decade, kernels are used as similarity functions within inner-product based algorithms such as the SVM for NLP problems and especially for text categorization. In this paper, we present a semantic space constructed from latent concepts. The concepts are extracted using the Latent Semantic Analysis (LSA). To take into account of the specificity of each document category, we use the local LSA to define the global semantic space. Furthermore, we propose a weighted semantic kernel for the global space. The experimental results of the kernel, on text categorization tasks, show that this kernel performs better than global LSA kernels and especially for small LSA dimensions.</resume>
<thematiques>Support Vector Machine, Latent Semantic Analysis, Natural Language Processing, Kernel, Computer Science [cs]/Machine Learning [cs.LG], Computer Science [cs]/Numerical Analysis [cs.NA], Computer Science [cs]/Artificial Intelligence [cs.AI], Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art114" titre="A Scalable Approach for Computing Semantic Relatedness using Semantic Web Data">
<auteurs>DIEFENBACH Dennis, USBECK Ricardo, SINGH Kamal, et al.</auteurs>
<titre_de_la_source>Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics</titre_de_la_source>
<date_de_publication>2016</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01341205</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>199511960B</numero_national_de_structure_de_recherche>
<doi>10.1145/2912845.2912864</doi>
<reference_hal>hal-01341205</reference_hal>
<references_archives_oai>oai:HAL:hal-01341205v1</references_archives_oai>
</informations_complementaires>
<resume>Computing semantic relatedness is an essential operation for many natural language processing (NLP) tasks, such as Entity Linking (EL) and Question Answering (QA). It is still challenging to find a scalable approach to compute the semantic relatedness using Semantic Web data. Hence, we present for the first time an approach to pre-compute the semantic relatedness between the instances, relations, and classes of an ontology, such that they can be used in real-time applications.</resume>
<thematiques>question answering, semantic web, Entity Linking, semantic relatedness, Computer Science [cs]/Artificial Intelligence [cs.AI], Computer Science [cs]/Databases [cs.DB], Computer Science [cs]/Computation and Language [cs.CL], Computer Science [cs]/Web</thematiques>
</article>
<article id="art115" titre="Arabic Named Entity Recognition Process using Transducer Cascade and Arabic Wikipedia">
<auteurs>BENMESMIA Fatma, HADDAR Kais, MAUREL Denis, et al.</auteurs>
<titre_de_la_source>RANLP 2015</titre_de_la_source>
<date_de_publication>2015</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01376732</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>201220254T</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01376732</reference_hal>
<references_archives_oai>oai:HAL:hal-01376732v1</references_archives_oai>
</informations_complementaires>
<resume>Transducers namely transducer cascades are used in several NLP-applications such as Ara-bic named entity recognition (ANER). To ex-periment and evaluate an ANER process, a weight coverage corpus is necessary. In this pa-per, we propose an ANER method based on transducer cascade. The proposed transducer cascade is generated with the CasSys tool inte-grated in Unitex linguistic platform. The exper-imentation of our method is done on a Wikipe-dia corpus. The Wikipedia text format is ob-tained with Kiwix tool. The experiment results are satisfactory.</resume>
<thematiques>Cascade of transducers, Wikipe-dia, Arabic named entities, Unitex, CasSys, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art116" titre="Découverte de nouvelles entités et relations spatiales à partir d’un corpus de SMS">
<auteurs>ZENASNI Sarah, KERGOSIEN Eric, ROCHE Mathieu, et al.</auteurs>
<titre_de_la_source>TALN 2016</titre_de_la_source>
<date_de_publication>2016</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01358585</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200615319E, 200718239Z</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01358585</reference_hal>
<references_archives_oai>oai:HAL:hal-01358585v1</references_archives_oai>
</informations_complementaires>
<resume>Dans le contexte des masses de données aujourd’hui disponibles, de nombreux travaux liés à l’analyse de l’information spatiale s’appuient sur l’exploitation des données textuelles. La communication médiée (SMS, tweets, etc.) véhiculant des informations spatiales prend une place prépondérante. L’objectif du travail présenté dans cet article consiste à extraire ces informations spatiales à partir d’un corpus authentique de SMS en français. Nous proposons un processus dans lequel, dans un premier temps, nous extrayons de nouvelles entités spatiales (par exemple, motpellier , montpeul àassocier au toponyme Montpellier ). Dans un second temps, nous identifions de nouvelles relations spatiales qui précèdent les entités spatiales (par exemple, sur, par, pres, etc.). La tâche est difficile et complexe en raison de la spécificité du langage SMS qui repose sur une écriture peu standardisée (apparition de nombreux lexiques, utilisation massive d’abréviations, variation par rapport à l’écrit classique , etc.). Les expérimentations qui ont été réalisées à partir du corpus 88milSMS mettent en relief la robustesse de notre système pour identifier de nouvelles entités et relations spatiales.</resume>
<thematiques>Etiquetage grammatical, Mesure de Similarité, Relations spatiales, Entités spatiales, Corpus de SMS. RÉSUMÉ : Dans le contexte des masses, Computer Science [cs]</thematiques>
</article>
<article id="art117" titre="Named Entity Resources - Overview and Outlook">
<auteurs>EHRMANN Maud, NOUVEL Damien, ROSSET Sophie</auteurs>
<titre_de_la_source>Language Resources and Evaluation</titre_de_la_source>
<date_de_publication>2016</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal-inalco.archives-ouvertes.fr/hal-01359441</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>199814049J, 197217542U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01359441</reference_hal>
<references_archives_oai>oai:HAL:hal-01359441v1</references_archives_oai>
</informations_complementaires>
<resume>Recognition of real-world entities is crucial for most NLP applications. Since its introduction some twenty years ago, named entity processing has undergone a significant evolution with, among others, the definition of new tasks (e.g. entity linking) and the emergence of new types of data (e.g. speech transcriptions, micro-blogging). These pose certainly new challenges which affect not only methods and algorithms but especially linguistic resources. Where do we stand with respect to named entity resources? This paper aims at providing a systematic overview of named entity resources, accounting for qualities such as multilingualism, dynamicity and interoperability, and to identify shortfalls in order to guide future developments.</resume>
<thematiques>named entity, linguistic resources, NE typologies, annotated corpora, evaluation, linked data, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art118" titre="Approches à base de fréquences pour la simplification lexicale">
<auteurs>LIGOZAT Anne-Laure, GROUIN Cyril, GARCIA-FERNANDEZ Anne, et al.</auteurs>
<titre_de_la_source>TALN-RECITAL 2013</titre_de_la_source>
<date_de_publication>2013</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00838354</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112516E, 197217542U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00838354</reference_hal>
<references_archives_oai>oai:HAL:hal-00838354v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>simplification lexicale, fréquence lexicale, modèle de langue, Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art119" titre="Enriching Morphological Lexica through Unsupervised Derivational Rule Acquisition">
<auteurs>WALTHER Géraldine, NICOLAS Lionel</auteurs>
<titre_de_la_source>WoLeR 2011at ESSLLI : International Workshop on Lexical Resources</titre_de_la_source>
<date_de_publication>2011</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.inria.fr/inria-00617064</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112497J</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>inria-00617064</reference_hal>
<references_archives_oai>oai:HAL:inria-00617064v1, oai:HAL:halshs-00751191v1, oai:HAL:halshs-00751190v1</references_archives_oai>
</informations_complementaires>
<resume>In a morphological lexicon, each entry combines a lemma with a specific inflection class, often defined by a set of inflection rules. Therefore, such lexica usually give a satisfying account of inflectional operations. Derivational information, however, is usually badly covered. In this paper we introduce a novel approach for enriching morphological lexica with derivational links between entries and with new entries derived from existing ones and attested in large-scale corpora, without relying on prior knowledge of possible derivational processes. To achieve this goal, we adapt the unsupervised morphological rule acquisition tool MorphAcq (Nicolas et al., 2010) in a way allowing it to take into account an existing morphological lexicon developed in the Alexina framework (Sagot, 2010), such as the Lefff for French and the Leffe for Spanish. We apply this tool on large corpora, thus uncovering morphological rules that model derivational operations in these two lexica. We use these rules for generating derivation links between existing entries, as well as for deriving new entries from existing ones and adding those which are best attested in a large corpus. In addition to lexicon development and NLP applications that benefit from rich lexical data, such derivational information will be particularly valuable to linguists who rely on vast amounts of data to describe and analyse these specific morphological phenomena.</resume>
<thematiques>Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art120" titre="La structuration prosodique et les relations syntaxe/ prosodie dans le discours politique">
<auteurs>FELDHAUSEN Ingo, DELAIS-ROUSSARIE Elisabeth</auteurs>
<titre_de_la_source>Actes de la conférence conjointe JEP-TALN-RECITAL 2012</titre_de_la_source>
<date_de_publication>2012</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://halshs.archives-ouvertes.fr/halshs-00751111</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112497J</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>halshs-00751111</reference_hal>
<references_archives_oai>oai:HAL:halshs-00751111v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>C-ACTI</thematiques>
</article>
<article id="art121" titre="GLAWI, a free XML-encoded Machine-Readable Dictionary built from the French Wiktionary">
<auteurs>SAJOUS Franck, HATHOUT Nabil</auteurs>
<titre_de_la_source>eLex</titre_de_la_source>
<date_de_publication>2015</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://halshs.archives-ouvertes.fr/halshs-01191012</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200711926M</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>halshs-01191012</reference_hal>
<references_archives_oai>oai:HAL:halshs-01191012v1</references_archives_oai>
</informations_complementaires>
<resume>This article introduces GLAWI, a large XML-encoded machine-readable dictionary automatically extracted from Wiktionnaire, the French edition of Wiktionary. GLAWI contains 1,341,410 articles and is released under a free license. Besides the size of its headword list, GLAWI inherits from Wiktionnaire its original macrostructure and the richness of its lexicographic descriptions: articles contain etymologies, definitions, usage examples, inflectional paradigms, lexical relations and phonemic transcriptions. The paper first gives some insights on the nature and content of Wiktionnaire, with a particular focus on its encoding format, before presenting our approach, the standardization of its microstructure and the conversion into XML. First intended to meet NLP needs, GLAWI has been used to create a number of customized lexicons dedicated to specific uses including linguistic description and psycholinguistics. The main one is GLÀFF, a large inflectional and phonological lexicon of French. We show that many more specific on demand lexicons can be easily derived from the large body of lexical knowledge encoded in GLAWI.</resume>
<thematiques>French Machine-Readable Dictionary, Free Lexical Resource, Wiktionary, Wiktionnaire, Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art122" titre="Automatic extraction of paraphrastic phrases from medium size corpora">
<auteurs>POIBEAU Thierry</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2004</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00009168</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112433P</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00009168</reference_hal>
<references_archives_oai>oai:HAL:hal-00009168v1</references_archives_oai>
</informations_complementaires>
<resume>This paper presents a versatile system intended to acquire paraphrastic phrases from a representative corpus. In order to decrease the time spent on the elaboration of resources for NLP system (for example Information Extraction, IE hereafter), we suggest to use a machine learning system that helps defining new templates and associated resources. This knowledge is automatically derived from the text collection, in interaction with a large semantic network.</resume>
<thematiques>paraphrase, semantic network, natural language processing, semantics, Computer Science [cs]/Artificial Intelligence [cs.AI]</thematiques>
</article>
<article id="art123" titre="Majorization-minimization procedures and convergence of SQP methods for semi-algebraic and tame programs">
<auteurs>BOLTE Jérôme, PAUWELS Edouard</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination>442-465</pagination>
<lien>https://hal.inria.fr/hal-01069737</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>199517454Y, 200717400M</numero_national_de_structure_de_recherche>
<doi>10.1287/moor.2015.0735</doi>
<reference_hal>hal-01069737</reference_hal>
<references_archives_oai>oai:HAL:hal-01069737v1</references_archives_oai>
</informations_complementaires>
<resume>In view of solving nonsmooth and nonconvex problems involving complex constraints (like standard NLP problems), we study general maximization-minimization procedures produced by families of strongly convex sub-problems. Using techniques from semi-algebraic geometry and variational analysis -in particular Lojasiewicz inequality- we establish the convergence of sequences generated by this type of schemes to critical points. The broad applicability of this process is illustrated in the context of NLP. In that case critical points coincide with KKT points. When the data are semi-algebraic or real analytic our method applies (for instance) to the study of various SQP methods: the moving balls method, Sl1QP, ESQP. Under standard qualification conditions, this provides -to the best of our knowledge- the first general convergence results for general nonlinear programming problems. We emphasize the fact that, unlike most works on this subject, no second-order assumption and/or convexity assumptions whatsoever are made. Rate of convergence are shown to be of the same form as those commonly encountered with first order methods.</resume>
<thematiques>SQP methods, Sl1QP, Moving balls method, Extended Sequential Quadratic Method, KKT points, KL inequality, Nonlinear programming, Converging methods, Tame optimization, Mathematics [math]/Optimization and Control [math.OC]</thematiques>
</article>
<article id="art124" titre="Dictionnaires électroniques et étiquetage syntactico-sémantique">
<auteurs>BUVET Pierre-André, CARTIER Emmanuel, ISSAC Fabrice, et al.</auteurs>
<titre_de_la_source>TALN 2007</titre_de_la_source>
<date_de_publication>2007</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://halshs.archives-ouvertes.fr/halshs-00168405</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200712563E</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>halshs-00168405</reference_hal>
<references_archives_oai>oai:HAL:halshs-00168405v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>étiqueteur sémantique, désambiguïsation, dictionnaire électronique, LMF, XML, XPATH, Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art125" titre="MACAON: an NLP tool suite for processing word lattices">
<auteurs>NASR Alexis, BÉCHET Frédéric, REY Jean-François, et al.</auteurs>
<titre_de_la_source>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Systems Demonstrations</titre_de_la_source>
<date_de_publication>2011</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal-amu.archives-ouvertes.fr/hal-01194259</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>201220273N, 200612270R, 201220259Y, 199712638E</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01194259</reference_hal>
<references_archives_oai>oai:HAL:hal-00702442v1, oai:HAL:hal-01194259v1</references_archives_oai>
</informations_complementaires>
<resume>no abstract</resume>
<thematiques>Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art126" titre="XMG: eXtending MetaGrammars to MCTAG">
<auteurs>PARMENTIER Yannick, KALLMEYER Laura, LICHTE Timm, et al.</auteurs>
<titre_de_la_source>Conférence sur le Traitement Automatique des Langues Naturelles - TALN 2007</titre_de_la_source>
<date_de_publication>2007</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.inria.fr/inria-00160400</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>198912571S</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>inria-00160400</reference_hal>
<references_archives_oai>oai:HAL:inria-00160400v1</references_archives_oai>
</informations_complementaires>
<resume>In this paper, we introduce an extension of the XMG system (eXtensible MetaGrammar) in order to allow for the description of Multi-Component Tree Adjoining Grammars. In particular, we introduce the XMG formalism and its implementation, and show how the latter makes it possible to extend the system relatively easily to different target formalisms, thus opening the way towards multi-formalism.</resume>
<thematiques>syntactic formalisms, tree-based grammars, metagrammars, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art127" titre="Un système analogique visuo-gestuel pour la graphie de la LS">
<auteurs>DANET Claire, DECOURVILLE Raphaël, MILETITCH Roman, et al.</auteurs>
<titre_de_la_source>Atelier Traitement Automatique des Langues des Signes, TALN 2010</titre_de_la_source>
<date_de_publication>2010</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00612504</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112427H</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00612504</reference_hal>
<references_archives_oai>oai:HAL:hal-00612504v1</references_archives_oai>
</informations_complementaires>
<resume>This work constitutes a contribution to the emergence of a common writing for French Sign Language. Our approach, while in line with previously conducted research (the LSscript project made a review of past works in the field of research of a graphical formalism for LSF), brings the reflexion into a multidisciplinary context. Our assumption is that writing and sign language share a same visual-gestural modality. Our hypothesis is as follows : in its execution, the gestural sign contains a readable graphic trace. In order to question our assumption, we use multi-angle photography to constitute a corpus of signs in which all three dimensions of these traces are visible. This article describes the principles and hypothesis, the techniques for building the corpus, and the validation protocol of our hypothesis.</resume>
<thematiques>Graphical system, gesture visualization, iconicization (transcription), French Sign Language, Système graphique, visualisation du geste, iconicisation (transcription), Langue des Signes Française, Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art128" titre="Fusion of hyperspectral and panchromatic images using multiresolution analysis and nonlinear PCA band reduction">
<auteurs>LICCIARDI GiorgioAntonino, KHAN M.M., CHANUSSOT Jocelyn, et al.</auteurs>
<titre_de_la_source>IEEE International Geoscience and Remote Sensing Symposium (IGARSS 2011)</titre_de_la_source>
<date_de_publication>2011</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00696054</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200711885T, 200012161Y</numero_national_de_structure_de_recherche>
<doi>10.1109/IGARSS.2011.6049466</doi>
<reference_hal>hal-00696054</reference_hal>
<references_archives_oai>oai:HAL:hal-00696054v1, oai:HAL:hal-00786285v1</references_archives_oai>
</informations_complementaires>
<resume>This paper presents a novel method for the enhancement of spatial quality of Hyperspectral (HS) images while making use of a high resolution panchromatic (PAN) image. Due to the high number of bands the application of a pansharpening technique to HS images may result in an increase of the computational load and complexity. Thus a dimensionality reduction preprocess, compressing the original number of measurements into a lower dimensional space, becomes mandatory. To solve this problem we propose a pansharpening technique combining both dimensionality reduction and fusion, exploited by non-linear Principal Component Analysis (NLPCA) and Indusion respectively, to enhance the spatial resolution of a hyperspectral image.</resume>
<thematiques>Engineering Sciences [physics]/Signal and Image processing, Computer Science [cs]/Signal and Image Processing</thematiques>
</article>
<article id="art129" titre="Grapheme To Phoneme Conversion - An Arabic Dialect Case">
<auteurs>HARRAT Salima, MEFTOUH Karima, ABBAS Mourad, et al.</auteurs>
<titre_de_la_source>Spoken Language Technologies for Under-resourced Languages</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.inria.fr/hal-01067022</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>198912571S</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01067022</reference_hal>
<references_archives_oai>oai:HAL:hal-01067022v1</references_archives_oai>
</informations_complementaires>
<resume>We aim to develop a speech translation system between Modern Standard Arabic and Algiers dialect. Such a system must include a Text-to-Speech module which itself must include a grapheme-phoneme converter. Algiers dialect is an Arabic dialect concerned by the most problems of Modern Standard Arabic in NLP area. Furthermore, it could be considered as an under-resourced language because it is a vernacular language for which no substantial corpus exists. In this paper we present a grapheme-to-phoneme converter for this language. We used a rule based approach and a statistical approach, we got an accuracy of 92% VS 85% despite the lack of resource for this language.</resume>
<thematiques>Modern Standrad Arabic, Algiers Dialect, Grapheme-To-Phoneme, Statistical Machine Translation, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art130" titre="Reduced RLT constraints for polynomial programming">
<auteurs>CAFIERI Sonia, HANSEN Pierre, LÉTOCART Lucas, et al.</auteurs>
<titre_de_la_source>EWMINLP10, European Workshop on Mixed Integer Nonlinear Programming</titre_de_la_source>
<date_de_publication>2010</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal-enac.archives-ouvertes.fr/hal-00938438</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112433P, 200519331V</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00938438</reference_hal>
<references_archives_oai>oai:HAL:hal-00938438v1</references_archives_oai>
</informations_complementaires>
<resume>An extension of the reduced Reformulation-Linearization Technique constraints from quadratic to general polynomial programming problems with linear equality constraints is presented and a strategy to improve the associated convex relaxation is proposed.</resume>
<thematiques>polynomial, MINLP, sBB, convex relaxation, RLT, Mathematics [math]/Optimization and Control [math.OC]</thematiques>
</article>
<article id="art131" titre="De la transcription horizontale et multimodale instrumentée à la transcription verticale pour la publication papier">
<auteurs>VEYRIER Clair-Antoine</auteurs>
<titre_de_la_source>DISH2011 Doctorants, informatique et sciences humaines, rencontre satellite de TALN2011</titre_de_la_source>
<date_de_publication>2011</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00745097</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200711930S</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00745097</reference_hal>
<references_archives_oai>oai:HAL:hal-00745097v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art132" titre="NLP model and tools for detecting and interpreting metaphors in domain-specific corpora">
<auteurs>BEUST Pierre, FERRARI Stéphane, PERLERIN Vincent</auteurs>
<titre_de_la_source>Proceedings of the Corpus Linguistics 2003 conference</titre_de_la_source>
<date_de_publication>2003</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00324699</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200012161Y</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00324699</reference_hal>
<references_archives_oai>oai:HAL:hal-00324699v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>Cognitive science/Computer science</thematiques>
</article>
<article id="art133" titre="Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 2: TALN">
<auteurs>ANTONIADIS Georges, BLANCHON Hervé, SÉRASSET Gilles</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2012</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00959240</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source></type_de_la_source>
<numero_national_de_structure_de_recherche>200711886U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00959240</reference_hal>
<references_archives_oai>oai:HAL:hal-00959240v1</references_archives_oai>
</informations_complementaires>
<resume>no abstract</resume>
<thematiques>Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art134" titre="Le projet BabyTalk : génération de texte à partir de données hétérogènes pour la prise de décision en unité néonatale">
<auteurs>PORTET François, GATT Albert, HUNTER Jim, et al.</auteurs>
<titre_de_la_source>TALN 2009, Senlis, 24-26 juin 2009</titre_de_la_source>
<date_de_publication>2009</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00959220</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200711886U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00959220</reference_hal>
<references_archives_oai>oai:HAL:hal-00959220v1</references_archives_oai>
</informations_complementaires>
<resume>no abstract</resume>
<thematiques>Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art135" titre="The iMAG concept: multilingual access gateway to an elected Web sites with incremental quality increase through collaborative post-edition of MT pretranslations.">
<auteurs>BOITET Christian, HUYNH Cong-Phap, NGUYEN Hong-Thai, et al.</auteurs>
<titre_de_la_source>TALN 2010</titre_de_la_source>
<date_de_publication>2010</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00959167</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200711886U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00959167</reference_hal>
<references_archives_oai>oai:HAL:hal-00959167v1</references_archives_oai>
</informations_complementaires>
<resume>no abstract</resume>
<thematiques>Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art136" titre="Zulu: an Interactive Learning Competition">
<auteurs>DELAHIGUERA Colin, DAVID Combe, JANODET Jean-Christophe</auteurs>
<titre_de_la_source>FSMNLP 2009</titre_de_la_source>
<date_de_publication>2009</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00476802</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>199511960B, 200812282V</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00476802</reference_hal>
<references_archives_oai>oai:HAL:hal-00476802v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>Computer Science [cs]/Machine Learning [cs.LG]</thematiques>
</article>
<article id="art137" titre="Repérage de structures thématiques dans des textes">
<auteurs>FERRET Olivier, GRAU Brigitte, MINEL Jean-Luc, et al.</auteurs>
<titre_de_la_source>TALN 2001</titre_de_la_source>
<date_de_publication>2001</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://halshs.archives-ouvertes.fr/halshs-00097828</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112501N, 197217542U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>halshs-00097828</reference_hal>
<references_archives_oai>oai:HAL:halshs-00097828v1</references_archives_oai>
</informations_complementaires>
<resume>To improve the results of automatic summarization or semantic filtering systems concerning thematic coherence, we propose a model which combines a statistic analysis system identifying thematic breaks and a linguistic analysis system identifying discourse frames.</resume>
<thematiques>Cadre thématique, cohérence thématique, exploration contextuelle, Humanities and Social Sciences/Linguistics, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art138" titre="Relever des critères pour la distinction automatique entre les documents médicaux scientifiques et vulgarisés en russe et en japonais">
<auteurs>KRIVINE Sonia, TOMIMITSU Masaru, GRABAR Natalia, et al.</auteurs>
<titre_de_la_source>13Ã¨me conférence sur le Traitement Automatique des Langues Naturelles (TALN 2006)</titre_de_la_source>
<date_de_publication>2006</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00456780</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200812282V</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00456780</reference_hal>
<references_archives_oai>oai:HAL:hal-00456780v1</references_archives_oai>
</informations_complementaires>
<resume>no abstract</resume>
<thematiques>Computer Science [cs]/Document and Text Processing, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art139" titre="Systèmes Question-Réponse et EuroWordNet">
<auteurs>JACQUIN Christine, MONCEAUX Laura, DESMONTILS Emmanuel</auteurs>
<titre_de_la_source>13ème conférence sur le Traitement Automatique des Langues Naturelles (TALN 2006)</titre_de_la_source>
<date_de_publication>2006</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00456757</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200812282V</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00456757</reference_hal>
<references_archives_oai>oai:HAL:hal-00456757v1</references_archives_oai>
</informations_complementaires>
<resume>no abstract</resume>
<thematiques>Computer Science [cs]/Document and Text Processing, Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art140" titre="Yet Another Ranking Function for Automatic Multiword Term Extraction">
<auteurs>LOSSIO-VENTURA JuanAntonio, JONQUET Clement, ROCHE Mathieu, et al.</auteurs>
<titre_de_la_source>PolTAL: Natural Language Processing</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>http://hal-lirmm.ccsd.cnrs.fr/lirmm-01068556</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200718239Z, 199111950H</numero_national_de_structure_de_recherche>
<doi>10.1007/978-3-319-10888-9_6</doi>
<reference_hal>lirmm-01068556</reference_hal>
<references_archives_oai>oai:HAL:lirmm-01068556v1</references_archives_oai>
</informations_complementaires>
<resume>Term extraction is an essential task in domain knowledge acquisition. We propose two new measures to extract multiword terms from a domain-specific text. The first measure is both linguistic and statistical based. The second measure is graph-based, allowing assessment of the importance of a multiword term of a domain. Existing measures often solve some problems related (but not completely) to term extraction, e.g., noise, silence, low frequency, large-corpora, complexity of the multiword term extraction process. Instead, we focus on managing the entire set of problems, e.g., detecting rare terms and overcoming the low frequency issue. We show that the two proposed measures outperform precision results previously reported for automatic multiword extraction by comparing them with the state-of-the-art reference measures.</resume>
<thematiques>Graphs, Graph Mining, Text Mining, BioNLP, Automatic Term Extraction, Computer Science [cs]/Artificial Intelligence [cs.AI], Computer Science [cs]/Document and Text Processing, Computer Science [cs]/Bioinformatics [q-bio.QM], Life Sciences [q-bio]/Quantitative Methods [q-bio.QM]</thematiques>
</article>
<article id="art141" titre="From binary collocations to grammatically extended collocations: Some insights in the semantic field of emotions in French.">
<auteurs>TUTIN Agnès, KRAIF Olivier</auteurs>
<titre_de_la_source>Mémoires de la Société néophilologique de Helsinki</titre_de_la_source>
<date_de_publication>2016</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01337486</lien>
<issn>0355-0192</issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>199113164C</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01337486</reference_hal>
<references_archives_oai>oai:HAL:hal-01337486v1</references_archives_oai>
</informations_complementaires>
<resume>Collocations have long been recorded by lexicographers but the theorization of thiskind of phraseological unit is quite recent. In line with the lexicological “continental”tradition of collocation, we propose a set of semantic and syntactic criteria to delimit clearly this phenomenon. We then show that, in the semantic field of emotions, collocations are more regular than expected because a) they are especially productive in specific constructions,  b) few semantic relations, as shown by Explanatory and Combinatorial Dictionary Lexical Functions, are very productive,  c) many collocates are combined with large paradigms of bases. We also highlight the importance of grammatical elements in collocations, which are essential but have failed to receive the attention they require. We demonstrate that corpus-based linguistic studies can offer interesting insights for this issue, through NLP tools such as the Lexicoscope.</resume>
<thematiques>collocations, French, grammatcal properties, collocation extraction, Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art142" titre="Federating clustering and cluster labelling capabilities with a single approach based on feature maximization: French verb classes identification with IGNGF neural clustering.">
<auteurs>LAMIREL Jean-Charles, FALK Ingrid, GARDENT Claire</auteurs>
<titre_de_la_source>Neurocomputing</titre_de_la_source>
<date_de_publication>2015</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.inria.fr/hal-01074277</lien>
<issn>0925-2312</issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>198912571S</numero_national_de_structure_de_recherche>
<doi>10.1016/j.neucom.2014.02.060</doi>
<reference_hal>hal-01074277</reference_hal>
<references_archives_oai>oai:HAL:hal-01074277v1</references_archives_oai>
</informations_complementaires>
<resume>Classifications which group together verbs and a set of shared syntactic and semantic properties have proven to be useful in both linguistics and Natural Language Processing tasks. However, most existing approaches for automatically acquiring verb classes fail to associate the verb classes produced with an explicit characterisation of the syntactic and semantic properties shared by the class elements. We propose a novel approach to verb clustering which addresses this shortcoming and permits building verb classifications whose classes group together verbs, subcategorisation frames and thematic grids. Our approach involves the use of a recent neural clustering method called IGNGF (Incremental Growing Neural Gas with Feature maximization). The use of a standard distance measure for determining a winner is replaced in IGNGF by feature maximisation measure relying on the features of the data that are associated with clusters during learning. A main advantage of the method is that maximised features used by IGNGF during learning can also be exploited in a final step for accurately labelling the resulting clusters. In this paper, we exploit IGNGF for the unsupervised classification of French verbs and evaluate the obtained clusters (i.e., verb classes) in two different ways. The first way is a quantitative analysis of the clustering process relying on a usual gold standard and on complementary unbiased clustering quality indexes. The second way is a qualitative analysis of the cluster labelling process. Relying on an adapted gold standard, we evaluate the capacity of the IGNGF clusters labels (i.e., subcategorisation frames and thematic grids) to be exploited for bootstraping a VerbNet-like classification for French. Both analyses clearly highlight the advantages of the approach.</resume>
<thematiques>Verb classification, Cluster labelling, NLP, Clustering, Incremental learning, Neural networks, Computer Science [cs]/Document and Text Processing, Computer Science [cs]/Machine Learning [cs.LG], Computer Science [cs]/Information Retrieval [cs.IR]</thematiques>
</article>
<article id="art143" titre="Application des Vecteurs Sémantiques à la Fouille de Texte">
<auteurs>CHAUCHÉ Jacques</auteurs>
<titre_de_la_source>TALN: Traitement Automatique des Langues Naturelles</titre_de_la_source>
<date_de_publication>2005</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>http://hal-lirmm.ccsd.cnrs.fr/lirmm-00106016</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>199111950H</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>lirmm-00106016</reference_hal>
<references_archives_oai>oai:HAL:lirmm-00106016v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>SEMANTIC ANALYSIS, Computer Science [cs]</thematiques>
</article>
<article id="art144" titre="Deliverable D5.1: Report on method and language for the production of the augmented document representations">
<auteurs>ALPHONSE Erick, AUBIN Sophie, DERIVIÈRE Julien, et al.</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2006</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00101549</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source></type_de_la_source>
<numero_national_de_structure_de_recherche>200112433P</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00101549</reference_hal>
<references_archives_oai>oai:HAL:hal-00101549v1</references_archives_oai>
</informations_complementaires>
<resume>This deliverable describes the specifications of the Natural Language Processing line to be developed within the Work Package 5 (WP5) in the Alvis Project. The WP5 is in charge of the NLP aspects of the Information Retrieval process. Its main objective is to enrich and normalise the crawled documents provided by the WP7 prior to their indexing. In this respect, WP5 output is a set of annotated documents, which are provided either to WP2 as input of its probabilistic model, which in turn delivers document to WP3 for indexing, or to WP6 as acquisition corpora. This deliverable presents the objectives of the WP5 process for document annotation. It introduces the basic notions of linguistics that are used in the following. The core section presents the various types of annotations that WP5 is expected to produce and describes the format in which these annotations are encoded. In addition to that, this deliverable also gives an overview of the WP5 processing line, its architecture for the various languages studied in Alvis and the various NLP components that produce annotated documents.</resume>
<thematiques>Natural Language processing, linguistic annotations, XML, stand-off annotations, Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art145" titre="Coreference. Annotation, Resolution and Evaluation in Polish">
<auteurs>OGRODNICZUK Maciej, KATARZYNA Glowinska, MATEUSZ Kopec, et al.</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01174653</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source></type_de_la_source>
<numero_national_de_structure_de_recherche>201220254T</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01174653</reference_hal>
<references_archives_oai>oai:HAL:hal-01174653v1</references_archives_oai>
</informations_complementaires>
<resume>This book presents work on coreference understanding, annotation and resolution of a Slavic language which can be applied to natural language processing in computers and software using English and other languages. By presenting the steps of building a coreference-related component of the NLP toolset, the volume serves as a reference book on state-of-the art methods in coreference projects for new languages and a tutorial for NLP practitioners.</resume>
<thematiques>Coreference Resolution, corpus annotation, Polish, Computer Science [cs]/Computation and Language [cs.CL], Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art146" titre="A step towards the detection of semantic variants of terms in technical documents">
<auteurs>HAMON Thierry, NAZARENKO Adeline, GROS Cécile</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>1998</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00009174</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112433P</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00009174</reference_hal>
<references_archives_oai>oai:HAL:hal-00009174v1</references_archives_oai>
</informations_complementaires>
<resume>This paper reports the results of a preliminary experiment on the detection of semantic variants of terms in a French technical document. The general goal of our work is to help the structuration of terminologies. Two kinds of semantic variants can be found in traditional terminologies : strict synonymy links and fuzzier relations like see-also. We have designed three rules which exploit general dictionary information to infer synonymy relations between complex candidate terms. The results have been examined by a human terminologist. The expert has judged that half of the overall pairs of terms are relevant for the semantic variation. He validated an important part of the detected links as synonymy. Moreover, it appeared that numerous errors are due to few mis-interpreted links: they could be eliminated by few exception rules.</resume>
<thematiques>Terminology, synonymy, NLP, Computer Science [cs]/Artificial Intelligence [cs.AI]</thematiques>
</article>
<article id="art147" titre="Towards modeling Arabic lexicons compliant LMF in OWL-DL">
<auteurs>LHIOUI Malek, HADDAR Kais, ROMARY Laurent</auteurs>
<titre_de_la_source>Terminology and Knowledge Engineering 2014</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01005854</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200719778X</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01005854</reference_hal>
<references_archives_oai>oai:HAL:hal-01005854v1</references_archives_oai>
</informations_complementaires>
<resume>Elaborating reusable lexical databases and especially making interoperability operational are crucial tasks effecting both Natural Language Processing (NLP) and Semantic Web. With this respect, we consider that modeling Lexical Markup Framework (LMF) in Web Ontology Language Description Logics (OWL-DL) can be a beneficial attempt to reach these aims. This proposal will have large repute since it concerns the reference standard LMF for modeling lexical structures. In this paper, we study the requirement for this suggestion. We first make a quick presentation of the LMF framework. Next, we define the three ontology definition sublanguages that may be easily used by specific users: OWL Lite, OWL-DL and OWL Full. After comparing of the three, we have chosen to work with OWL-DL. We then define the ontology language OWL and describe the steps needed to model LMF in OWL. Finally, we apply this model to develop an instance for an Arabic lexicon.</resume>
<thematiques>Computer Science [cs]/Computation and Language [cs.CL]</thematiques>
</article>
<article id="art148" titre="Mot et traitement automatique des langues">
<auteurs>CARTIER Emmanuel, ISSAC Fabrice</auteurs>
<titre_de_la_source>Le Français Moderne - Revue de linguistique Française</titre_de_la_source>
<date_de_publication>2009</date_de_publication>
<numero>1</numero>
<pagination></pagination>
<lien>https://halshs.archives-ouvertes.fr/halshs-00410912</lien>
<issn>0015-9409</issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Collection</type_de_la_source>
<numero_national_de_structure_de_recherche>200712563E</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>halshs-00410912</reference_hal>
<references_archives_oai>oai:HAL:halshs-00410912v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>TALN, Mot, Typographie, Segmentation, Racine, Morphème, Séquence figée, Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art149" titre="Investigating domain-independent NLP techniques for precise target selection in video hyperlinking">
<auteurs>SIMON Anca-Roxana, GUINAUDEAU Camille, SÉBILLOT Pascale, et al.</auteurs>
<titre_de_la_source>ISCA/IEEE Workshop on Speech, Language and Audio in Multimedia</titre_de_la_source>
<date_de_publication>2014</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01053698</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200012163A, 197217542U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01053698</reference_hal>
<references_archives_oai>oai:HAL:hal-01053698v1</references_archives_oai>
</informations_complementaires>
<resume>Automatic generation of hyperlinks in multimedia video data is a subject with growing interest, as demonstrated by recent work undergone in the framework of the Search and Hyperlinking task within the Mediaeval benchmark initiative. In this paper, we compare NLP-based strategies for precise target selection in video hyperlinking exploiting speech material, with the goal of providing hyperlinks from a specified anchor to help information retrieval. We experimentally compare two approaches enabling to select short portions of videos which are relevant and possibly complementary with respect to the anchor. The first approach exploits a bipartite graph relating utterances and words to find the most relevant utterances. The second one uses explicit topic segmentation, whether hierarchical or not, to select the target segments. Experimental results are reported on the Mediaeval 2013 Search and Hyperlinking dataset which consists of BBC videos, demonstrating the interest of hierarchical topic segmentation for precise target selection.</resume>
<thematiques>Multimedia hyperlinking, topic segmentation, link analysis, information retrieval, Computer Science [cs]/Multimedia [cs.MM]</thematiques>
</article>
<article id="art150" titre="A Bayesian approach combining surface clues and linguistic knowledge: Application to the anaphora resolution problem">
<auteurs>WEISSENBACHER Davy, NAZARENKO Adeline</auteurs>
<titre_de_la_source>Recent Advances in Natural Language Processing</titre_de_la_source>
<date_de_publication>2007</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00162114</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200112433P</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00162114</reference_hal>
<references_archives_oai>oai:HAL:hal-00162114v1</references_archives_oai>
</informations_complementaires>
<resume>In NLP, A traditional distinction opposes the linguistically-based systems and the knowledge-poor ones which mainly rely on surface clues. Each approach has its drawbacks and its advantages. In this paper, we propose a new method which is based on Bayes Networks and allows to combine both types of information. As a case study, we focus on the specific task of pronominal anaphora resolution which is known as a difficult NLP problem. We show that our bayesian system performs better than state-of-the art anaphora resolution ones.</resume>
<thematiques>Computer Science [cs]/Document and Text Processing</thematiques>
</article>
<article id="art151" titre="The TXM Platform: Building Open-Source Textual Analysis Software Compatible with the TEI Encoding Scheme">
<auteurs>HEIDEN Serge</auteurs>
<titre_de_la_source>24th Pacific Asia Conference on Language, Information and Computation</titre_de_la_source>
<date_de_publication>2010</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://halshs.archives-ouvertes.fr/halshs-00549764</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200311862K</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>halshs-00549764</reference_hal>
<references_archives_oai>oai:HAL:halshs-00549764v1</references_archives_oai>
</informations_complementaires>
<resume>This paper describes the rationale and design of an XML-TEI encoded corpora compatible analysis platform for text mining called TXM. The design of this platform is based on a synthesis of the best available algorithms in existing textometry software. It also relies on identifying the most relevant open-source technologies for processing textual resources encoded in XML and Unicode, for efficient full-text search on annotated corpora and for statistical data analysis. The architecture is based on a Java toolbox articulating a full-text search engine component with a statistical computing environment and with an original import environment able to process a large variety of data sources, including XML-TEI, and to apply embedded NLP tools to them. The platform is distributed as an open-source Eclipse project for developers and in the form of two demonstrator applications for end users: a standard application to install on a workstation and an online web application framework.</resume>
<thematiques>xml-tei corpora, search engine, statistical analysis, textometry, open-source, Humanities and Social Sciences/Methods and statistics, Computer Science [cs]/Document and Text Processing, Statistics [stat]/Applications [stat.AP], Computer Science [cs]/Computation and Language [cs.CL], Computer Science [cs]/Digital Libraries [cs.DL], Humanities and Social Sciences/Linguistics</thematiques>
</article>
<article id="art152" titre="Alvis NLP Platform">
<auteurs>DERIVIÈRE Julien, HAMON Thierry</auteurs>
<titre_de_la_source></titre_de_la_source>
<date_de_publication>2006</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-00090753</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source></type_de_la_source>
<numero_national_de_structure_de_recherche>200112433P</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-00090753</reference_hal>
<references_archives_oai>oai:HAL:hal-00090753v1</references_archives_oai>
</informations_complementaires>
<resume></resume>
<thematiques>NLP, Annotation, Web documents, NLP architecture, Computer Science [cs]/Artificial Intelligence [cs.AI]</thematiques>
</article>
<article id="art153" titre="Robustesse de la RAP à la parole expressive âgée vs. typique : contexte de commandes dans un habitat intelligent">
<auteurs>AMAN Frédéric, AUBERGÉ Véronique, VACHER Michel</auteurs>
<titre_de_la_source>JEP-TALN-RECITAL 2016, Atelier TALPA</titre_de_la_source>
<date_de_publication>2016</date_de_publication>
<numero></numero>
<pagination></pagination>
<lien>https://hal.archives-ouvertes.fr/hal-01326499</lien>
<issn></issn>
<informations_complementaires>
<type_de_publication>&prepublication;</type_de_publication>
<type_de_la_source>Evènement</type_de_la_source>
<numero_national_de_structure_de_recherche>200711886U</numero_national_de_structure_de_recherche>
<doi></doi>
<reference_hal>hal-01326499</reference_hal>
<references_archives_oai>oai:HAL:hal-01326499v1</references_archives_oai>
</informations_complementaires>
<resume>La commande vocale a été identifiée comme un mode d’interaction très intéressant dans l’habitat intelligent, qu’elle soit adressée directement à l’habitat ou à une interface robotique, aussi bien en ce qui concerne le confort que dans le domaine de l’assistance aux personnes âgées. Cependant, même lorsque le sujet peut contrôler sa production pour respecter strictement une référence imposée, dans le contexte naturel de l’usage quotidien « naturel » les productions vocales sont inévitablement souvent expressives. Dans cet article, à partir d’un corpus de parole émue actée/neutre collecté par élicitation,nous observons une chute significative de performance d’un système de RAP générique pour la parole émue par rapport à la voix neutre et nous évaluons le gain intéressant apporté par une adaptation du système. Nous concluons sur la nécessité de prendre en compte cette adaptationdans le développement d’un système vocal destiné à l’assistance aux personnes.</resume>
<thematiques>distress call, Ambient Assisted Living, Expressive speech, Parole avec affect, appel de détresse, habitat intelligent pour la santé, Computer Science [cs]/Other [cs.OH]</thematiques>
</article>
</articlesTAL>